# Регуляризация регрессии {#regularization}

Мы говорили о том, что частой проблемой при подборе модели линейной регрессии является мультиколлинеарность предикторов. Напомним, чем это чревато.

В случае абсолютно линейной связи коэффициент модели просто не вычислится --- в аутпуте будет `NA`. Как правило, это намёк на то, чтобы проверить данные --- возможно, у вас есть одна и та же переменная, записанная по-разному (например, рост в метрах и сантиметрах). В случае высоких (но меньших единицы) корреляций проблема подбора коэффициентов решается с помощью методов численной оптимизации, однако это может приводить к смещённым оценкам коэффициентов модели, а также большим ошибкам в оценках коэффициентов.

Поэтому с мультиколлинеарностью надо бороться. Мы уже обсуждали [метод служебной регрессии](https://angelgardt.github.io/hseuxlab-wlm2021/book/multiple-linear.html#staff_reg). Теперь обсудим ещё один способ.

В ходе подбора модели мы получаем оценки коэффициентов модели $\hat \beta_j = b_j$:

$$
\hat y_i = b_0 + b_1 x_{i1} + b_2 x_{i2} + \dots + b_p x_{ip}
$$

В случае мультиколлинеарности оценки ряда коэффициентов могут быть завышены. Значит нужно их «штрафовать»! Вернее, не сами коэффициенты, а сумму квадратов ошибок модели:

$$
RSS + \text{штраф} \rightarrow \underset{\hat \beta}{\min}
$$

Но штрафовать можно по-разному --- и мы будем получать разные результаты.


## Варианты штрафа

Штраф состоит из суммы оценок коэффициентов модели. Однако здесь снова та же проблема, что и с дисперсией --- оценки могут быть как положительные, так и отрицательные, поэтому для того, чтобы их сумма не обнулилась, нужно каким-то образом избавиться от знака. Варианта как обычно два --- поэтому мы получаем два способа подсчёта штрафа.

Напомним себе, что

$$
RSS = \sum_{i=1}^n (y_i - \hat y_i)
$$

Мы можем взять квадраты коэффициентов модели, тогда получим следующее:

$$
\sum_{i=1}^n (y_i - \hat y_i) + \lambda \sum_{j=1}^p b_j^2 \rightarrow \underset{b}{\min}
$$

Такой подход называется **ridge-регрессия**.

Мы можем взять модули коэффициентов модели, тогда получим такое:

$$
\sum_{i=1}^n (y_i - \hat y_i) + \lambda \sum_{j=1}^p |b_j| \rightarrow \underset{b}{\min}
$$
Такой подход называется **LASSO-регрессия**.

Существует и промежуточный вариант:

$$
\sum_{i=1}^n (y_i - \hat y_i) + \lambda_1 \sum_{j=1}^p |b_j| + \lambda_2 \sum_{j=1}^p b_j^2 \rightarrow \underset{b}{\min}
$$

Он называется **метод эластичной сети**.

Во всех выражениях выше $\lambda$ --- это *штрафной коэффициент*, который и определяет величину штрафа.


### Пример работы штрафного коэффициента

Чтобы наглядно увидеть, как работает регуляризация и на что влияет штрафной коэффициент, подберем коэффициенты для самой простой линейной модели на «игрушечных» данных. Подбор проведём двумя методами --- обычным МНК и ridge-регрессией.

Пусть у нас есть такие данные:

|$y_i$|$x_i$|
|:-:|:-:|
|10|1|
|20|1|
|30|2|

Модель возьмём самую простую --- $y_i = \beta x_i + \varepsilon_i$. Даже без интерсепта. Подберём оценку коэффициента $\hat \beta = b$ методом наименьших квадратов.

Напомним, что в МНК мы минимизируем RSS, которая в даном случае будет выражаться так:

$$
RSS = \sum_{i=1}^n (y_i - \hat y_i)^2 = \sum_{i=1}^n (y_i - bx_i)^2 = \\
= \sum_{i=1}^n (y_i^2 - 2 y_i bx_i + b^2 x_i^2) = \\
= \sum_{i=1}^n y_i^2 - 2b \sum_{i=1}^n x_i y_i + b^2 \sum_{i=1}^n x_i^2 = Q(b)
$$

Мы получили выражение для RSS, от которого теперь можем взять производную, чтобы затем приравнять её к нулю и найти мнимум функции:

$$
Q'(b) = -2 \sum_{i=1}^n x_i y_i + 2 b \sum_{i=1}^n x_i^2 = 0
$$

$$
b = \frac{\sum_{i=1}^n x_i y_i}{\sum_{i=1}^n x_i^2} \\
b = \frac{10 + 20 + 60}{1 + 1 + 4} = \frac{90}{6} = 15 = \hat \beta_{\text{МНК}}
$$

Итак, мы получили оценку коэффициента методом наименьших квадратов. Теперь посмотрим, что изменится, если мы будем использовать подход регуляризованной регрессии.

В ridge-регрессии, которую мы возьмём для примера, минимизируется RSS плюс сумма квадратов коэффициентов модели. В нашем случае:

$$
RSS + \lambda b^2 \rightarrow \underset{b}{\min}
$$

Функция $Q_R(b)$ очень похожа на функцию $Q(b)$ и отличается от неё наличием дополнительного слагаемого $\lambda b^2$, которое и выражает штраф:

$$
Q_R(b) = \sum_{i=1}^n y_i^2 - 2b \sum_{i=1}^n x_i y_i + b^2 \sum_{i=1}^n x_i^2 + \lambda b^2
$$

Производная будет выглядеть так:

$$
Q'_R(b) = -2 \sum_{i=1}^n x_i y_i + 2 b \sum_{i=1}^n x_i^2 + 2 b \lambda = 0
$$

Пока всё логично. Магия начинается сейчас.

$$
b = \frac{\sum_{i=1}^n x_i y_i}{\sum_{i=1}^n x_i + \lambda}
$$

Штрафной коэффициент $\lambda$ оказывается в знаменателе, а значит он будет оказывать прямое влияения на величину коэффициента. Скажем, если мы возьмем его значение $\lambda = 240$, то получим следующее:

$$
b = \frac{90}{6 + 240} < 1
$$

Таким образом, значение коэффициента было «оштрафовано» и в результате мы подучили значительно меньшее его значение. Конечно, этот пример игрушечный, но с механизмом регуляризации вы теперь знакомы.


## Регуляризованная регрессия в R

### Данные

Сегодня мы работаем с [данными про менеджеров-продажников](https://github.com/angelgardt/hseuxlab-wlm2021/blob/master/book/wlm2021-book/data/cpi.xlsx?raw=true). Данные содержат информацию о некоторых характеристиках менеджеров (количественные шкалы `Fx`, `Cs`, `Sy`, `Sp`, `In`, `Em`, `Re`, `Sc`, `Ie`, `Do`), а также сумму, на которую менеджер напродавал. Наша задача узнать, какие характеристики менеджеров сильнее всего связаны с их эффективностью в продажах.

Подключаем пакеты, которые нам понадобятся:

```{r}
library(tidyverse)
library(glmnet) # для регуляризации
```

Грузим данные:

```{r}
cpi <- readxl::read_xlsx('data/cpi.xlsx')
str(cpi)
```

Немного подредактируем данные, чтобы проще записывать модели --- уберем колонку `group`, которая нам не понадобится в анализе:

```{r}
cpi %>% select(-group) -> cpi
```


### Особенности синтаксиса пакета `glmnet`

Функция, позволяющая строить модель регуляризованной регрессии, находится в пакете `glmnet` и называется `cv.glmnet()`. При подборе коэффициентов модели она проводит [кросс-валидацию](https://en.wikipedia.org/wiki/Cross-validation_(statistics)), что позволяет получить ещё более точные оценки параметров.

У функции несколько непривычный синтаксис: она требует на вход отдельно вектор целевой переменной и отдельно матрицу предикторов. Придется немного подготовить даннные.

Выносим целевую переменную в отдельный вектор:

```{r}
y <- cpi$prodazhi
```

Создаем матрицу предикторов:

```{r}
X <- as.matrix(cpi %>% select(-prodazhi))
```

Теперь всё готово к тому, чтобы строить модели.


### Ridge-регрессия в R

Чтобы подобрать коэффициенты модели с помощью ridge-регрессии, надо указать в функции `cv.glmnet()` аргумент `alpha = 0`:

```{r}
model_ridge <- cv.glmnet(x = X, y = y, alpha = 0)
```

Модель зафитилась. Можно посмотреть интересный график с помощью функции `plot()`.

```{r plot_ridge}
plot(model_ridge)
```

На этом графике по оси $x$ оторажается логарифм[^5] штрафного коэффициента $\lambda$, по оси $y$ --- средняя квадратичная ошибка модели (MSE). Можно пронаблюдать, как в среднем меняется ошибка модели с увеличением штрафного коэффициента (красные точки). Серые «усы» показывают границы --- минимум и максимум --- ошибки на разных подвыборках кросс-валидации.

[^5]: Так просто удобнее визуализировать.

Первая вертикальная линия --- это значение штрафного коэффициента $\lambda_\min$, при котором значение MSE минимально. Обычно для анализа берут именно это значение. Но есть ещё одно --- вторая вертикальная линия ---- значение штрафного коэффициента $\lambda_{1\text{SE}}$, которое находится на расстоянии одной стандартной ошибки от минимального. Иногда в такой модели получаются более интересные результаты.

Числа над графиком обозначают количество предикторов, которое осталось в модели при данном значении штрафного коэффициента. Мы видим, что там всегда 10, а мы ввели в модель как раз 10 предикторов. Это справедливо, так как ridge-регрессия склонна оставлять все предикторы в модели, поэтому если ваша приоритетная задача --- получить точные оценки для всех коэффициентов, используйте этот тип регуляризации.

Вывести полученные коэффициенты можно с помощью функции `coef()`:

```{r}
coef(model_ridge, s = 'lambda.min') # для модели с минимальной ошибкой
coef(model_ridge, s = 'lambda.1se') # для модели с ошибкой на 1 SE большее от минимальной
```

Здесь нам доступны только сами оценки коэффициентов. К сожалению, после того, как мы вводим штраф в алгоритм подбора модели, мы теряем возможность тестировать статистические гипотезы. Поэтому мы не можем сказать, значимо ли влияние `In` с коэффициентом $-9$ или нет.

Но всё-таки можно кое-что сделать…



### LASSO-регрессия в R

Для LASSO-регрессии всё остаётся точно так же, за исключением того, что необходимо указать `alpha = 1`:

```{r}
model_lasso <- cv.glmnet(x = X, y = y, alpha = 1)
```

Посмотрим график:

```{r plot_lasso}
plot(model_lasso)
```

Ход движения линии изменился, что, в целом, ожидаемо, ведь мы использовали другой тип регуляризации. Значения всех элементов графика здесь абсолютно то же самое, что и в предыдущем случае. Интересно другое.

Теперь на графиков числа умпеньшаются по мере увеличения [логарифма] штрафного коэффициента. Это происходит потому, что LASSO-регрессия склонна занулять предикторы, поэтому если ваша приоритетная задача --- отбор предикторов, то ваш выбор --- именно этот тип регуляризации.

Посмотрим, что теперь покажет функция `coef()`:

```{r}
coef(model_lasso, s = 'lambda.min') # для модели с минимальной ошибкой
coef(model_lasso, s = 'lambda.1se') # для модели с ошибкой на 1 SE большее от минимальной
```

У ряда предикторов нет коэффициента, что говорит нам о том, что он был обнулён. Это можно рассматривать как аналог тестирования статистической значимости, конечно с большими оговорками. Но идейно: если коэффициент не обнулился, то он статистически значим, если однулился --- то не значим. Заметьте, что в зависимости от того, какой штрафной коэффициент мы выберем, у нас будет зануляться различное число коэффциентов.


Регуляризованная регрессия часто используется в индустрии, так как этот подход не усложняет саму модель и не вносит сложности в интерпретацию предикторов, однако вместе с тем помогает бороться со множеством проблем индустриальных данных, в частности, мультиколлинеарностью.

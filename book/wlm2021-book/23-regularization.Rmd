# Регуляризация регрессии {#regularization}

Мы говорили о том, что частой проблемой при подборе модели линейной регрессии является мультиколлинеарность предикторов. Напомним, чем это чревато.

В случае абсолютно линейной связи коэффициент модели просто не вычислится --- в аутпуте будет `NA`. Как правило, это намёк на то, чтобы проверить данные --- возможно, у вас есть одна и та же переменная, записанная по-разному (например, рост в метрах и сантиметрах). В случае высоких (но меньших единицы) корреляций проблема подбора коэффициентов решается с помощью методов численной оптимизации, однако это может приводить к смещённым оценкам коэффициентов модели, а также большим ошибкам в оценках коэффициентов.

Поэтому с мультиколлинеарностью надо бороться. Мы уже обсуждали [метод служебной регрессии](https://angelgardt.github.io/hseuxlab-wlm2021/book/multiple-linear.html#staff_reg). Теперь обсудим ещё один способ.

В ходе подбора модели мы получаем оценки коэффициентов модели $\hat \beta_j = b_j$:

$$
\hat y_i = b_0 + b_1 x_{i1} + b_2 x_{i2} + \dots + b_p x_{ip}
$$

В случае мультиколлинеарности оценки ряда коэффициентов могут быть завышены. Значит нужно их «штрафовать»! Вернее, не сами коэффициенты, а сумму квадратов ошибок модели:

$$
RSS + \text{штраф} \rightarrow \underset{\hat \beta}{\min}
$$

Но штрафовать можно по-разному --- и мы будем получать разные результаты.


## Варианты штрафа

Штраф состоит из суммы оценок коэффициентов модели. Однако здесь снова та же проблема, что и с дисперсией --- оценки могут быть как положительные, так и отрицательные, поэтому для того, чтобы их сумма не обнулилась, нужно каким-то образом избавиться от знака. Варианта как обычно два --- поэтому мы получаем два способа подсчёта штрафа.

Напомним себе, что

$$
RSS = \sum_{i=1}^n (y_i - \hat y_i)
$$

Мы можем взять квадраты коэффициентов модели, тогда получим следующее:

$$
\sum_{i=1}^n (y_i - \hat y_i) + \lambda \sum_{j=1}^p b_j^2 \rightarrow \underset{b}{\min}
$$

Такой подход называется **ridge-регрессия**.

Мы можем взять модули коэффициентов модели, тогда получим такое:

$$
\sum_{i=1}^n (y_i - \hat y_i) + \lambda \sum_{j=1}^p |b_j| \rightarrow \underset{b}{\min}
$$
Такой подход называется **LASSO-регрессия**.

Существует и промежуточный вариант:

$$
\sum_{i=1}^n (y_i - \hat y_i) + \lambda_1 \sum_{j=1}^p |b_j| + \lambda_2 \sum_{j=1}^p b_j^2 \rightarrow \underset{b}{\min}
$$

Он называется **метод эластичной сети**.







# Корреляционный анализ

До этого момента мы рассматривали только отдельные переменные и их характерики, однако в практике мы редко работаем только с одной переменной. Как правило, у нас есть многомерное пространство признаков, и нас интересуют взаимосвязи между ними.


## Ковариация

Мы хотим описать имеющиеся взаимосвязи как можно проще и опираясь на то, что у нас уже есть. Мы говорили, что дисперсия, или вариация, заключает в себе информацию об изменчивости признака. Если мы хотим исследовать взаимосвязь между признаками, то логично будет посмотреть, как изменяется один из признаков при изменении другого --- иначе говоря, рассчитать *совместную изменчивость признаков*, или **ко-вариацию (covariance)**.

Как мы её будем считать? Подумаем графически. Расположим две переменные на осях и сопоставим каждому имеющемуся наблюдению точку на плоскости.

КАРТИНКА

Отметим средние значения по обеим переменным.

КАРТИНКА

Заметим, что если наши наблюдения по переменной $x_1$ отклоняются в большую сторону, то они отклоняются в большую сторону и по переменной $x_2$. Аналогично, если они будут отклоняться в меньшую сторону по $x_1$, то в меньшую же сторону они будут отклоняться и по $x_2$.

КАРТИНКА

Получается, мы можем на основании согласованности отклонений уже заключить о направлении связи. *Произведение отклонений по обеим величинам будет положительно, если отклонения сонаправленны*. Запишем это математически.

$$
(\bar x_1 - x_{i1}) (\bar x_2 - x_{i2}) > 0 \Leftarrow \big( (\bar x_1 - x_{i1}) > 0 \wedge (\bar x_2 - x_{i2}) > 0 \big) \vee \big( (\bar x_1 - x_{i1}) < 0 \wedge (\bar x_2 - x_{i2}) < 0 \big)
$$

Соответственно, если отклонения будут направлены в разные стороны, из произведение будет отрицательным. Ну, осталось только понять, как совместные отклонения организованы в среднем --- это и будет ковариацией двух величин:

$$
\mathrm{cov}(X_1, X_2) = \frac{1}{n-1} \sum_{i=1}^n (\bar x_1 - x_{i1}) (\bar x_2 - x_{i2})
$$

<img class="taskimg" src="img/task.png">
<div class="task">
Что такое ковариация величины самой с собой ($\mathrm{cov}(X_1, X_1)$)? Докажите через выведение формулы.
</div>

Важно отметить, что ковариация улавливается только *линейную составляющую* взаимосвязи между признаками, поэтому если $\mathrm{cov}(X_1,X_2) = 0$, то мы можем сказать, что между переменными нет *линейной* взаимосвязи, однако это не значит, что между этими переменными нет никакой другой зависимости.

У ковариации есть два важных недостатка:

* это размерная величина, поэтому её значение зависит от единиц измерения признаков,
* она зависит от дисперсий признаков, поэтому по её значению можно определить только направление связи (прямая или обратная), однако ничего нельзая сказать о силе связи.

Поэтому нам нужно как-то модицифировать эту статистику, чтобы мы могли больше вытащить из её значения.


## Корреляция

Раз ковариация зависит от дисперсии, то можно сделать некоторые математические преобразования, чтобы привести эмпирические распределения к какому-то одному виду --- сделать так, чтобы они имели одинакое математическое ожидание (среднее) и одинаковую дисперсию. С этой задачей прекрасно справляется [стандартизация](#standartization). Напоминаю формулу:

$$
x_i^* = \frac{x_i - \bar x}{s}
$$
После такого преобразования математическое ожидание нашего распределения будет равно нуля, а стандартное отклонение --- единице. Это избавит нас от влияния дисперсии на значение ковариации. Ковариация двух стандартно нормально распределенных величин называется **корреляцией (correlation)**.

$$
\mathrm{cov}(X_1^*, X_2^*) = \frac{1}{n-1} \sum_{i=1}^n x_{i1} x_{i2} = \mathrm{corr}(X_1, X_2),
$$
где $X_1^*$ и $X_2^*$ --- стандартизированные величины $X_1$ и $X_2$ соответственно.

Корреляцию можно выразить через ковариацию:

$$
\mathrm{corr}(X_1, X_2) = \frac{1}{n-1} \sum_{i=1}^n \Big( \frac{\bar x_1 - x_{i1}}{s_1} \Big) \Big( \frac{\bar x_2 - x_{i2}}{s_2} \Big) = 
\frac{1}{s_1 s_2} \Big( \frac{1}{n-1} \sum_{i=1}^n (\bar x_1 - x_{i1})(\bar x_2 - x_{i2}) \Big) = \frac{\mathrm{cov}(X_1, X_2)}{s_1, s_2}
$$

Если внимательно всмотреться в формуле, то можно обнаружить, что корреляция это не что иное, как стандартизированное значение ковариации.

Коэффициент корреляции имеет четкие пределы изменения: $[-1; \,1]$. Крайнее левое значение говорит о том, что присутствует полная обратная линейная взаимосвязь, крайнее правое --- что присутствует полная прямая линейная взаимосвязь. Как и ковариация, корреляция ловит *только линейную составляющую* связи, поэтому нулевое значение корреляци показывает, что между переменными отсутствует *линейная взаимосвязь*. Это всё еще не значит, что связи нет вовсе.


### Тестирование статистической значимости коэффициента корреляции

### Доверительный интервал для коэффициента корреляции







# Корреляционный анализ {#correlation}

До этого момента мы рассматривали только отдельные переменные и их характерики, однако в практике мы редко работаем только с одной переменной. Как правило, у нас есть многомерное пространство признаков, и нас интересуют взаимосвязи между ними.


## Ковариация

Мы хотим описать имеющиеся взаимосвязи как можно проще и опираясь на то, что у нас уже есть. Мы говорили, что дисперсия, или вариация, заключает в себе информацию об изменчивости признака. Если мы хотим исследовать взаимосвязь между признаками, то логично будет посмотреть, как изменяется один из признаков при изменении другого --- иначе говоря, рассчитать *совместную изменчивость признаков*, или **ко-вариацию (covariance)**.

Как мы её будем считать? Подумаем графически. Расположим две переменные на осях и сопоставим каждому имеющемуся наблюдению точку на плоскости.

КАРТИНКА

Отметим средние значения по обеим переменным.

КАРТИНКА

Заметим, что если наши наблюдения по переменной $x_1$ отклоняются в большую сторону, то они отклоняются в большую сторону и по переменной $x_2$. Аналогично, если они будут отклоняться в меньшую сторону по $x_1$, то в меньшую же сторону они будут отклоняться и по $x_2$.

КАРТИНКА

Получается, мы можем на основании согласованности отклонений уже заключить о направлении связи. *Произведение отклонений по обеим величинам будет положительно, если отклонения сонаправленны*. Запишем это математически.

$$
(\bar x_1 - x_{i1}) (\bar x_2 - x_{i2}) > 0 \Leftarrow \big( (\bar x_1 - x_{i1}) > 0 \wedge (\bar x_2 - x_{i2}) > 0 \big) \vee \big( (\bar x_1 - x_{i1}) < 0 \wedge (\bar x_2 - x_{i2}) < 0 \big)
$$

Соответственно, если отклонения будут направлены в разные стороны, из произведение будет отрицательным. Ну, осталось только понять, как совместные отклонения организованы в среднем --- это и будет ковариацией двух величин:

$$
\mathrm{cov}(X_1, X_2) = \frac{1}{n-1} \sum_{i=1}^n (\bar x_1 - x_{i1}) (\bar x_2 - x_{i2})
$$

<img class="taskimg" src="img/task.png">
<div class="task">
Что такое ковариация величины самой с собой ($\mathrm{cov}(X_1, X_1)$)? Докажите через выведение формулы.
</div>

Важно отметить, что ковариация улавливается только *линейную составляющую* взаимосвязи между признаками, поэтому если $\mathrm{cov}(X_1,X_2) = 0$, то мы можем сказать, что между переменными нет *линейной* взаимосвязи, однако это не значит, что между этими переменными нет никакой другой зависимости.

У ковариации есть два важных недостатка:

* это размерная величина, поэтому её значение зависит от единиц измерения признаков,
* она зависит от дисперсий признаков, поэтому по её значению можно определить только направление связи (прямая или обратная), однако ничего нельзая сказать о силе связи.

Поэтому нам нужно как-то модицифировать эту статистику, чтобы мы могли больше вытащить из её значения.


## Корреляция

Раз ковариация зависит от дисперсии, то можно сделать некоторые математические преобразования, чтобы привести эмпирические распределения к какому-то одному виду --- сделать так, чтобы они имели одинакое математическое ожидание (среднее) и одинаковую дисперсию. С этой задачей прекрасно справляется [стандартизация](#standartization). Напоминаю формулу:

$$
x_i^* = \frac{x_i - \bar x}{s}
$$
После такого преобразования математическое ожидание нашего распределения будет равно нуля, а стандартное отклонение --- единице. Это избавит нас от влияния дисперсии на значение ковариации. Ковариация двух стандартно нормально распределенных величин называется **корреляцией (correlation)**.

$$
\mathrm{cov}(X_1^*, X_2^*) = \frac{1}{n-1} \sum_{i=1}^n x_{i1}^* x_{i2}^* = \mathrm{corr}(X_1, X_2),
$$
где $X_1^*$ и $X_2^*$ --- стандартизированные величины $X_1$ и $X_2$ соответственно.

Корреляцию можно выразить через ковариацию:

$$
\mathrm{corr}(X_1, X_2) = \frac{1}{n-1} \sum_{i=1}^n \Big( \frac{\bar x_1 - x_{i1}}{s_1} \Big) \Big( \frac{\bar x_2 - x_{i2}}{s_2} \Big) = 
\frac{1}{s_1 s_2} \Big( \frac{1}{n-1} \sum_{i=1}^n (\bar x_1 - x_{i1})(\bar x_2 - x_{i2}) \Big) = \frac{\mathrm{cov}(X_1, X_2)}{s_1 s_2}
$$

Если внимательно всмотреться в формуле, то можно обнаружить, что корреляция это не что иное, как стандартизированное значение ковариации.

Коэффициент корреляции имеет четкие пределы изменения: $[-1; \,1]$. Крайнее левое значение говорит о том, что присутствует полная обратная линейная взаимосвязь, крайнее правое --- что присутствует полная прямая линейная взаимосвязь. Как и ковариация, корреляция ловит *только линейную составляющую* связи, поэтому нулевое значение корреляци показывает, что между переменными отсутствует *линейная взаимосвязь*. Это всё еще не значит, что связи нет вовсе.


### Тестирование статистической значимости коэффициента корреляции

Оценку коэффициента корреляции мы получаем методом моментов, заменяя истинный момент $\rho_{ij}$ выборочным $r_{ij}$:

$$
\hat \rho_{ij} = \overline{\big( (X_{ki} - \bar X_i) (X_{kj} - \bar X_j) \big)} = r_{ij}
$$

Если в генеральной совокупности связь между признаками отсутствует, то есть $\rho_{ij} = 0$, будет ли равен нулю $r_{ij}$? Можно с уверенностью сказать, что не будет, так как выборочный коэффициент корреляции --- случайная величина. А мы помним, что вероятность принятия случайной величиной своего конкретного значения равна нулю.

Тогда необходимо протестировать статистическую гипотезу:

$$
H_0: \rho_{ij} = 0 \; \text{(линейной связи нет)} \\
H_1: \rho_{ij} \neq 0 \text{(наиболее частый вариант альтернативы)}
$$

Для проверки нулевой гипотезы используется следующая статистика:

$$
t = \frac{r_{ij}}{\sqrt{\frac{1 - r^2_{ij}}{n-2}}} \overset{H_0}{\thicksim} t(\nu = n-2)
$$

Вывод о статистической значимости коэффициента корреляции делается согласно [алгоритму тестировния статистических гипотез](#statestim).


### Доверительный интервал для коэффициента корреляции

С построением интервальной оценки коэффциента корреляции возникают некоторые сложности. Наша задача состоит в том, чтобы определить в каких границах будет лежать значение истинного коэффициента корреляции с заданной вероятностью:

$$
\mathrm{P} (\rho_{ij,\min} < \rho_{ij} < \rho_{ij,\max}) = \gamma
$$

Нам необходимо найти статистику, закон распределения корой известен, однако ранее упомянутся статистика не подходит, так как она имеет распределение Стьюдента, когда верна нулевая гипотеза об отсутствии связи. Если же мы строим интервальную оценку, нас интересует случай наличия связи.

Такую статистику искали долго, и её удалось найти, когда ввели определённое преобразование выборочного критерия корреяции --- *z-преобразования Фишера*:

$$
z(r_{ij}) = \frac{1}{2} \ln \frac{1 + r_{ij}}{1 - r_{ij}} \thicksim \mathrm{N}(\bar z_{ij}, \tfrac{1}{n-3}),
$$
где $n$ --- объём выборки, а $\bar z_{ij}$ получается расчётом по указанной формуле после подставления точечной оценки коэффициента корреляции.

Тогда интервальная оценка для величины $z_{ij, \mathrm{true}}$ приобретает такой вид:

$$
\mathrm{P} \Big( \bar z_{ij} - t_\gamma \sqrt{\tfrac{1}{n-3}} < z_{ij, \mathrm{true}} < \bar z_{ij} + t_\gamma \sqrt{\tfrac{1}{n-3}}  \Big) = \gamma = \Phi(t_\gamma)
$$

Далее путём обратного преобразования получаются значения границ интервала $(\rho_{ij,\min}, \; \rho_{ij,\max})$.



## Ковариация и корреляция в R

Запасёмся [данными](https://github.com/angelgardt/hseuxlab-wlm2021/blob/master/book/wlm2021-book/data/clean_dannye_norm.xlsx?raw=true). По ссылке скачается экселька.

```{r}
dehum <- readxl::read_xlsx('data/clean_dannye_norm.xlsx')
str(dehum)
```

Это данные исследования на тему дегуманизации убийц. Нас будут интересовать следующие шкалы:

* *DPa* --- шкала «одобрения казни»
* Шкала морального возмущения:
  - *GR_average* --- подшкала отвращения
  - *AR_average* --- подшкала гнева
  - *SR_average* --- подшкала презрения
* Шкала дегуманизации:
  - *animal_average* --- анималистическая дегуманизация
  - *machine_average* --- механистическая дегуманизация

Ковариация считается так:

```{r}
cov(dehum$GR_average, dehum$AR_average)
```

А корреляция так:

```{r}
cor(dehum$GR_average, dehum$AR_average)
```

Ещё мы можем построить соответствующий график, чтобы отобразить закономерность --- диаграмма рассеяния с линией тренда:

```{r}
library(tidyverse)
theme_set(theme_bw())
```
```{r}
dehum %>% ggplot(aes(GR_average, AR_average)) +
  geom_point() +
  geom_smooth(method = 'lm') # мы заинтересованы в отображении линейного компонента связи
```

Но это мы всё получали выборочные оценки коэффициентов. А как же тестировать гипотезы?

Легко и непринужденно! Просто дописать `test` в название функции:

```{r}
cor.test(dehum$GR_average, dehum$AR_average)
```

Что мы наблюдаем в аутпуте? Значение t-статистики, число степеней свободы, p-value для значения t-статистики, а также 95% доверительный интервал для коэффициента корреляции. Всё, что мы и хотели --- за одну команду!


### Коэффициенты корреляции для разных шкал

Дла разных шкал разработаны разные коэффициенты корреляции. Оценки коэффициентов будут рассчитываться по-разному, но логика тестирования статистических гипотез остаётся одинаковой.

|Переменная $X$|Переменная $Y$|Мера связи|
|:-:|:-:|:-:|
|Интервальная или отношений|Интервальная или отношений|Коэффициент Пирсона|
|Ранговая, интервальная или отношений|Ранговая, интервальная или отношений|Коэффициент Спирмена|
|Ранговая|Ранговая|Коэффициент Кенделла|

В функциях `cor()` и `cor.test()` требуемый коэффициент задаётся черед аргумент `method`:

```{r}
cor.test(dehum$DP, dehum$animal_average, method = 'kendall')
```

## Частный и множественный коэффициент корреляции

Если у нас два признака, то с ними всё достаточно понятно. А если признаком много? Тогда у нас могут быть сложные взаимосвязи, и возможен такой случай, что некоторый признак оказывает связан как с одним, так и с другим из интересующих нас. Таким образом, мы можем наблюдать *ложную корреляцию*. Чтобы избавиться от влияния сторонних признаков, используюся частные коэффициенты корреляции.

Функция `cor()` может возвращать не только оценку одного коэффициента корреляции, но и **корреляционную матрицу**, отобрадающую связи всех признаков со всеми. Например, продолжим работать со шкалой морального возмущения и изучим взаимосвязи внутри неё:

```{r}
dehum %>%
  select(GR_average, AR_average, SR_average) %>%
  cor()
```

В корреляционной матрице на главной диагонали стоят единицы, отражающай связь переменной в самой собой --- разумеется, она будет абсолютно линейная.

<img class="taskimg" src="img/code.png">
<div class="task">
А как посчитать *ковариационную* матрицу?
</div>

В общем виде корреляционная матрица имеет следующий вид:

$$
R = 
\begin{pmatrix}
1 & r_{12} & \dots & r_{1p} \\
r_{12} & 1 & \dots & r_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
r_{p1} & r_{p2} & \dots & 1
\end{pmatrix}
$$

Матрица, как можно заметить, симметрична относительно главной диагонали, так как $r_{ij} = r_{ji}$.

Её можно визуализироать, например, так:

```{r}
dehum %>%
  select(GR_average, AR_average, SR_average) %>%
  cor() %>% 
  corrplot::corrplot()
```

Но можно и усовершенствовать визуализацию, отобразив сами значения:

```{r}
dehum %>%
  select(GR_average, AR_average, SR_average) %>%
  cor() %>% 
  corrplot::corrplot.mixed()
```

На основе этой матрицы мы можем протестировать статистическую значимость каждого из коэффициентов (не забыв про поправки на множественные сравнения!):

```{r}
cor_tests <- correlation::correlation(
  dehum %>%
    select(GR_average, AR_average, SR_average),
  method = "spearman", # указываем, какой коэффициент нам нужен
  adjust = "holm" # и какую поправку будем использовать
)
cor_tests
```

Чтобы перенести их на график, нам нужно получить матрицу из p-значений:

```{r}
p_values <- corrplot::cor.mtest(dehum %>%
  select(GR_average, AR_average, SR_average), adjust = 'holm')$p
```
```{r}
dehum %>%
  select(GR_average, AR_average, SR_average) %>%
  cor() %>% 
  corrplot::corrplot(type = 'upper',
                     p.mat = p_values,
                     sig.level = 0.05)
```

Ну, у нас ничего не поменялось, так как коэффициенты все оказались значимы. Эх…

Но вот для примера на одно из встроенных датасетов:

```{r}
corrplot::corrplot(
  cor(mtcars),
  type = "upper",
  p.mat = corrplot::cor.mtest(mtcars)$p,
  sig.level = 0.01
)
```

Итак, возвращается к частному коэффициенту корреляции. Он определяется так:

$$
r_{ij, J(i,j)} = - \frac{A_{ij}}{\sqrt{A_{ii} A_{jj}}},
$$

где $A$ --- алгебраическое дополнение.

В общем виде это осознать сложно, поэтому давайте на примере трёх признаков.

$$
R =
\begin{pmatrix}
1 & r_{12} & r_{13} \\
r_{21} & 1 & r_{23} \\
r_{31} & r_{32} & 1
\end{pmatrix}
$$

$$
r_{12,3} = \frac{r_{12} - r_{13} \cdot r_{23}}{\sqrt{(1 - r^2_{23})(1-r^2{13})}}
$$

$$
H_0: \rho_{12,3} = 0 \\
H_1: \rho_{12,3} \neq 0 \\
t = \frac{r_{12,3} \sqrt{n-3}}{\sqrt{1 - r^2_{12,3}}} \overset{H_0}{\thicksim} t(\nu = n-3)
$$

Но слава богу, что в R это все делается в одну строку:

```{r}
ppcor::pcor(
  dehum %>% 
    select(GR_average, AR_average, SR_average)
)

ppcor::pcor.test(dehum$GR_average,
                 dehum$AR_average,
                 dehum$SR_average)
```

Хорошо, а если нас интересует связь одного признака с несколькими сразу? Тогда нам нужен множественный коэффициент корреляции. Он также вычисляется на основе корреляционной матрицы и определяется следующим образом. Пусть нас интересует связь первого признака со всеми остальными:

$$
R_1 = \sqrt{1 - \frac{\det R}{A_{11}}}
$$

Квадрат множественонго коэффициента корреляции называется *коэффициентом детерминации*[^1]. Он показывает, во-первых, степень тесноты связи данного признака со всеми остальными, но, кроме того, ещё и долю дисперсии данного признака, определяемую вариацией все остальных признаков, включенных в данную корреляционную модель.

Мы подробнее его изучим в следуюшей теме, а также увидим, где нам его найти, чтобы не считать руками.


## Другие корреляции

Можно коррелировать не только количественные и ранговые шкалы между собой, но и качественные тоже:

|Переменная $X$|Переменная $Y$|Мера связи|
|:-:|:-:|:-:|
|Дихотомическая|Дихотомическая|$\phi$-коэффициент|
|Дихотомическая|Ранговая|Рангово-бисериальный коэффициент|
|Дихотомическая|Интервальная или отношений|Бисериальный коэффициент|


### $\phi$-коэффициент

Этот коэффициент позволяет рассчитать корреляцию между двумы дихотомическими шкалами. Он основан на расчёте статистики $\chi^2$.

По двум дихотомическим переменным можно построить таблицу сопряженности. Разберемся на [котиках и пёсиках](https://raw.githubusercontent.com/angelgardt/hseuxlab-wlm2021/master/book/wlm2021-book/data/cats_n_dogs.csv):

```{r, include=FALSE}
tibble(id = 1:100,
       species = rep(c('cat', 'dog'), each = 50),
       size = sample(c('small', 'big'), size = 100, replace = TRUE, prob = c(1,2))) %>% write_csv('cats_n_dogs.csv')
```

```{r}
cats_n_dogs <- read_csv('https://raw.githubusercontent.com/angelgardt/hseuxlab-wlm2021/master/book/wlm2021-book/data/cats_n_dogs.csv')
```

```{r}
table(cats_n_dogs$species, cats_n_dogs$size)
```

По данной таблице можно рассчитать **критерий согласия Пирсона ($\chi^2$)**:

```{r}
chisq.test(table(cats_n_dogs$species, cats_n_dogs$size))
```







[^1]: Вы точно видели это словосочетание, когда сталкивались с линейной регресией.

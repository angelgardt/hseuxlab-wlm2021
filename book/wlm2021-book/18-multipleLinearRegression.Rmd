# Множественная линейная регрессия {#multiple_linear}

\newcommand{\X}{\boldsymbol{X}}
\newcommand{\y}{\boldsymbol{y}}
\newcommand{\b}{\boldsymbol{b}}
\newcommand{\e}{\boldsymbol{e}}
\newcommand{\T}{\mathrm{T}}

Связи двух переменных --- это, конечно, хорошо. Но на практике нас обычно интересуют более сложные закономерности. Их, как мы говорили, можно изучать и корреляционным анализом, но регрессия делает это как-то более элегантно.


## Множественная линейная регрессия с количественными предикторами

### Математическая модель

Итак, мы хотим изучить влияние[^1] нескольких независимых переменных (предикторов) на нашу зависимую (целевую) переменную. Модель в общем-то меняется не сильно --- просто добавляется ещё одно слагаемое:

$$
y_i = b_0 + b_1 x_{i1} + b_2 x_{i2} + e_i
$$

Теперь нам надо подбирать не два коэффициента, а три. Но, на самом деле, это ничего не меняет.

### Подбор модели

Конечно, если мы будем пытаться решить задачу аналитически, то там будут определённые изменения. Однако мы познакомились с матричными вычислениями и можем обратиться к ним.

В матричном виде модель будет записываться следующим образом:

$$
\y = \X \b + \e,
$$
где $\y$ --- всё ещё вектор нашей зависимой переменной, $\X$ --- всё ещё матрица независимых переменных, $\b$ --- всё ещё вектор коэффициентов модели, а $\e$ --- вектор ошибок (остатков) модели.

Отличие будет в том, как организованы внутри $\X$ и $\b$:

$$
\X = \begin{pmatrix}
1 & x_{11} & x_{12} \\
1 & x_{21} & x_{22} \\
1 & x_{31} & x_{32} \\
\vdots & \vdots & \vdots \\
1 & x_{n1} & x_{n2}
\end{pmatrix}; \quad

\b = \begin{pmatrix}
b_0 \\
b_1 \\
b_2
\end{pmatrix}
$$

Вычисление же коэффициентов будет осуществляться абсолютно аналогично простой линейной регрессии:

$$
\b = (\X^\T \X)^{-1}\X^\T \y
$$

Ясно, что не важно, сколько предикторов будет содержать модель --- вычисление коэффициентов будет работать одинаково. Однако по сравнению с простой линейной моделью здесь есть одна важная особеность.


#### Проблема мультиколлинеарности

Предикторы могут быть связаны не только с целевой перемненой, но и друг с другом, что обуславливает **проблему мультиколлинеарности**.

В чем она заключается? Посмотрим ещё раз на формулу выше, которая нам позволяет вычислить параметры модели: в ходе вычислений мы берем обратную матрицу. Если наши предикторы сильно коррелируют друг с другом ($\geq 0.8$), то в нашей матрице возникают линейно зависимые столбцы, а значит обратная матрица не будет существовать.

Чем это чревато? В случае абсолютно линейной связи коэффициент модели просто не вычислится --- в аутпуте будет `NA`. Как правило, это намёк на то, чтобы проверить данные --- возможно, у вас есть одна и та же переменная, записанная по-разному (например, рост в метрах и сантиметрах). В случае высоких (но меньших единицы) корреляций проблема подбора коэффициентов решается с помощью методов численной оптимизации, однако это может приводить к смещённым оценкам коэффициентов модели, а также большим ошибкам в оценках коэффициентов.

Поэтому с мультиколлинеарностью надо бороться. Вариантов существует много. Наиболее часто используемые: исключение коллинеарных переменных из модели, [метод служебной регресии](#staff_reg), методы уменьшения размерности (кластерный анализ, PCA и др.).


#### Подбор коэффициентов модели в R

Подбор модели осуществляется с помощью той же самой функции `lm()`, только в данном случае немного изменяется запись модели. Другие предикторы перечистяются через символ `+`, как и в математической записи модели.

Допустим, мы хотим учесть в модели [из прошлой главы] не только возраст, но и рост ребенка в предсказании ОФВ. Построим такую модель:

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
theme_set(theme_bw())
ofv <- read_delim('http://www.statsci.org/data/general/fev.txt', delim = '\t')
```

```{r}
model1 <- lm(FEV ~ Age + Height, ofv)
summary(model1)
```

Как мы видим, аутпут функции `summary()` практически не отличается от простой линейной регрессии, только угловых коэффициента у нас теперь два, а не один.


### Оценка качества модели

Качество модели оценивается аналогично простой линейной регрессии.

Прежде всего, мы смотрим на F-статистику и p-value, рассчитанный для неё. Если p-value меньше конвенционального значения 0.05, то мы делаем вывод о том, что модель в целом статистически значима.

Далее обращаем внимание на $R^2$, а лучше сразу на $R^2_{\mathrm{adj}}$ --- скорректированный коэффициент детерминации. Почему на него? Так как в нашей модели теперь несколько предикторов, то скорректированный [на количество предикторов] коэффициент детерминации для нас более информативен. В частности, по его значениям мы можем сравнивать модели с разным количеством предикторов. Как он корректируется? Вот так:

$$
R^2_{\mathrm{adj}} = 1 - (1 - R^2)\frac{n-1}{n-p}
$$

После оценки информативности модели мы переходим к тестированию значимости отдельных предикторов --- и здесь тоже всё аналогично простой линейной регрессии. Есть оценки значений коэффициентов, есть их стандатные ошибки, есть значение t-теста и p-values, рассчитанные для них. Со всем этим мы уже знакомы.



### Диагностика модели

#### Исследование мультиколлинеарности

Но если мы на секунду задумаемся… Мы ввели в модель предиктор «рост», который несомненно связан с ОФВ. Однако он также связан и с возрастом! А не принесли ли мы в нашу модель коллинеарных предикторов?

На этапе разведочного анализа при наличии большого числа предикторов полезно посмотреть на корреляционную матрицу предикторов --- что с чем связано. В нашем случае мы можем просто посчитать значение корреляции, так как у нас предикторов сейчас всего два:

```{r}
cor(ofv$Age, ofv$Height)
```

Ожидаемо высокая корреляция. Если корреляция предикторов друг с другом 0.8 и выше --- повод задуматься о мультиколлинеарности. У нас значение довольно близкое. Продолжим анализ.

Для того, чтобы выявить, есть ли таки у нас проблема мультикоолинеарности, или мы радостные и довольные можем наслаждаться результатами моделирования, необходимо вычислить **коэффициент вздутия дисперсии (variance inflation factor, VIF)**. Чтобы его вычислить, проводятся следующие операции:

* пусть у нас есть модель $y = b_0 + b_1 x_1 + b_2 x_2 + \dots + b_m x_m + e$
* строиться линейная регрессия, в которой один из предикторов регрессируется всеми другими. Например, для первого предиктора:

$$
x_1 = \alpha_0 + \alpha_2 x_2 + \dots \alpha_m x_m
$$

* вычисляется коэффициент детерминации для данной модели $R_i$
* вычисляется VIF для коэффциента $b_i$:

$$
\mathrm{VIF}_i = \frac{1}{1 - R^2_i}
$$

Все эти вычисления реализованы в функции `vif()` из пакета `car`:

```{r}
car::vif(model1)
```

Пороговым значением для вынесение вердикта о наличии мультиколлинеарности считается 3 (иногда 2). Мы этот вердикт всё же вынесем, и будем с мультиколлинеарностью бороться.


### Доработка модели {#staff_reg}

Как уже отмечалось выше, способов борьбы с мультиколлинеарностью существует несколько. Сейчас мы рассмотрим **метод служебной регрессии**.

В чём логика этого метода? Наши предикторы связаны друг с другом, но всё же корреляция меньше единицы (что так-то ожидаемо и логично). То есть, часть изменчивости роста может быть объяснена изменчивостью возраста, а часть, конечно же, останется случайной изменчивостью. Звучит как регрессия, не так ли?

Итак, мы можем построить служебную регрессионную модель, которая учтёт часть извенчивости роста, связанную с возрастом --- будем считать возраст основным предиктором для нашей главной модели. Всё то, что возрастом объяснено не будет, составит остатки модели служебной регрессии, которые мы сможем внести как предиктор в модель.

Давайте по порядку. Строим служебную регрессию:

```{r}
model_anc <- lm(Height ~ Age, ofv)
```

Записываем остатки модели в новую переменную датасета:

```{r}
ofv$resHeight <- model_anc$residuals
```

Заменяем в главной модели рост на «остатки по росту»:

```{r}
model2 <- lm(FEV ~ Age + resHeight, ofv)
summary(model2)
```

Коэффициенты, конечно, получились другие, но модель осталась статистически значима, более того, остатки служебной регрессии в качестве предиктора тоже оказались статистически значимы --- значит есть зависимость ОФВ от той изменчивости роста, которая не объясняется возрастом.

И самое главное, что больше нет мультиколлинеарности:

```{r}
car::vif(model2)
```

***

Для множественной линейной регрессии также можно строить диагностические графики, которые будут интерпретироваться аналогично графика простой линейной регрессии. Например, для модели `model2` графики будут выглядеть так:

```{r}
plot(model2)
```

<img class="taskimg" src="img/question.png">
<div class="task">
Оцените, стала ли модель лучше после включения в неё второго предиктора. Сравните `model2` и `model` из предыдущей главы.
</div>



## Множественная линейная регрессия с количественными и категориальными предикторами

Когда мы имеем дело с количественными предикторами, то всё более-менее ясно --- есть зависимость между двумя количественными переменными, описываемая прямой. А что делать, если среди предикторов появляются категориальные переменные?

Посмотрим на датасет, с которым мы работаем, внимательно:

```{r}
str(ofv)
```

Есть две тестовые переменые: `Smoker` и `Sex`.

```{r}
table(ofv$Smoker)
table(ofv$Sex)
```

Они категориальные. Вполне можно ожидать, что, например, у курящих и некурящих ОФВ будет различаться, так как это показатель косвенно свидетельствующий о состоянии бронхиального дерева. Как включить это в модель?

### Математическая модель

С точки зрения влияние категориального предиктора на целевую переменную будет выражаться в различном базовом уровне, то есть intercept, для разных категорий наблюдений. В модели это фиксируется так:

$$
y_i = b_0 + b_1 I_1 + b_2 x_2 + b_3 x_2
$$

В данном случае представлена модель с одним категориальным ($I_1$) и двумя количественными ($x_1$, $x_2$) предикторами.

Переменная $I$ --- это переменная индикатор, которая обладает следующим свойством:

* $I_1 = 0$, если наблюдение относится к категории 1,
* $I_1 = 1$, если наблюдение относится к категории 2.

Таким образом, у нас получается как бы две модели в одной:

* для наблюдений категории 1 модель принимает следующий вид: $y_i = b_0 + b_2 x_2 + b_3 x_2$,
* а для наблюдений категории 2 вот такой: $y_i = (b_0 + b_1) + b_2 x_2 + b_3 x_2$.

Коэффициент $b_1$ показывает разницу в базовых уровнях между двумя категориями (группами) наблюдений.


### Подбор модели

С точки зрения подбора модели кардинальных изменений не происходит:

```{r}
model3 <- lm(FEV ~ Age + resHeight + Smoker, ofv)
```

А вот на аутпут `summary()` придётся посмотреть чуть внимательнее:

```{r}
summary(model3)
```

Отличия мы находим в таблице с предикторами. Здесь мы видим, что у `Age` и у `resHeight` коэффициенты остались как и прежде, но добавилась ещё одна строка --- `SmokerNon`. Так как наша переменная содержит только два уровня, baseline какой-то из двух групп уходит в intercept --- в данном случае это `SmokerCurrent`. Коэффициент при `SmokerNon` показывается, на сколько отличается baseline группы `SmokerNon` по сравнению со `SmokerCurrent`. Итак, объем форсированного выхода на первую секунду у некурящих детей на 0.11 литра больше, чем у курящих. Результат, вполне согласующийся со здравым смыслом.

Оценка и диагностика модели производится аналогично тому, как мы это делали на предыдущих моделях.



## Модели со взаимодействием предикторов

Но можно ведь не только уловить различия в базовом уровне количественны предикторов в раз личных группах наблюдений, но и разную степень их связи с целевой переменной в этих группах. Для этоо нужно ввести **взаимодействие предикторов** в модель. 

### Математическая модель

В данном случае модель будет выглядеть так:

$$
y_i = b_0 + b_1 I_1 + b_2 x_2 + b_3 I_1 x_2
$$

Для простоты здесь один количественный и один категориальный предиктор. Переменная $I_1$ снова выступает здесь индикатором. То есть наша модель как бы снова содержит в себе две «подмодели»:

* для наблюдений категории 1 ($I_1 = 0$) модель принимает следующий вид: $y_i = b_0 + b_2 x_2$,
* а для наблюдений категории 2 ($I_1 = 1$) вот такой: $y_i = (b_0 + b_1) + (b_2 + b_3) x_2$.

Теперь у нас не только есть поправочный коэффициент на baseline, но и поправочный угловой коэффициент для количественного предиктора.

### Подбор модели

Взаиможействие предикторов можно указать двумя способами:

```{r}
model4 <- lm(FEV ~ Age + Smoker + Age:Smoker, ofv) # : указывает на дополнительный угловой коэффициент для категории Smoker
model4 <- lm(FEV ~ Age * Smoker, ofv) # эквивалентна предыдущей записи, но короче
```

Обе команды построят идентичные модели. Взглянем на `summary()`:

```{r}
summary(model4)
```

Intercept содержит baseline для уровня `Current` переменой `Smoker`. Коэффициент при `SmokerNon`, как и в предыдущей модели, показывает, насколько отличается baseline для уровня `Non` переменной `Smoker` от базового уровня категории `Current` этой же переменной. Коэффициент при `Age` --- это угловой коэффициент регрессионной прямой для категории `SmokerCurrent`, в то время как запись `Age:SmokerNon` показывается, то это добавочный угловой коэффициент для группы некурящих респондентов. Как видите, всё аналогично тому, как это задаётся в математической модели, поэтому если вы помните её, интерпретация результатов анализа не составляет особого труда.



## Сравнение моделей

Часто нам надо сравнить две модели друг с другом. Зачем? Например, чтобы понять, можно ли данную модель упростить, ведь мы хотим найти [наиболее простую и интерпретируемую модель](https://ru.wikipedia.org/wiki/Бритва_Оккама), не так ли?

Как уже упоминалось выше, можно это сделать по значению скорректированного коэффициента детерминации, но можно использовать и другие показатели. Есть способ сравнить информативность двух моделей статистическим тестом --- **частным F-критерием**. Делается это с помощью функции `anova()`, в которую необходимо передать сравниваемые модели.

Допустим мы хотим сравнить `model2` и `model3` --- первая содержит два количественных предиктора, а вторая ещё и категориальный.

```{r}
anova(model2, model3)
```

Здесь мы смотрим на значение F-статистики и p-value --- впрочем, как и обычно при тестировании статистических гипотез. Если модели статистически значимо различаются, то влияние предиктора, который присутствует в одной модели, но отсутствует в другой, статистически значимо. В данном случае мы видим, что обе модели в общем-то одинаковы по своей информативности --- то есть введение предиктора `Smoker` не особо-то и улучшает модель, даже несмотря на то, что он статистически значим. ~~Но мы получили значение p-value, которое говорит о том, что модели различаются на уровне статистической тенденции. Возможно, стоит обратить на это внимание.~~

Частный F-критерий можно рассчитать и с помощью другой функции --- `drop1()`. Допустим, мы решили регрессировать нашу целевую переменную по всем переменным датасета сразу да ещё и со всеми взаиможействиями:

```{r}
model5 <- lm(FEV ~ Age * Height * Sex * Smoker, ofv)
summary(model5)
```

В итоге мы получили, конечно, что-то значимое, но совершенно не интерпретабельное. Надо пытаться упрощать.

```{r}
drop1(model5, test = 'F')
```

Функция `drop1()` исключается по одному предиктору из модели, начиная со взаиможействий самого высокого порядка и сразу тестирует гипотезу о различии моделей. Необходимо в её аргументах указать `test = "F"`, так как сейчас нас интересует именно F-критерий. Эта функция умеет считать ещё и другие --- с ними мы столкнемся в следующих главах.

В данном случае мы видим, что при исключении из модели взаимодействия третьего уровня (то есть взаиможействия четырёх предикторов) информативность модели не изменится --- а значит модель можно упростить.

Ещё одна полезная функция --- это `update()`, чтобы не переписывать модель заново, а просто указать, какую её составляющую надо удалить:

```{r}
model5.1 <- update(model5, . ~ . -Age:Height:Sex:Smoker)
```

Синтаксис `. ~ . -x` говорит о том, что «оставь формулу модели той же, но удали из неё предиктор `x`».

Снова протестиируем модель с помощью частного F-критерия:
```{r}
drop1(model5.1, test = "F")
```

Видим, что при удалении взаиможействий `Age:Height:Smoker` и `Age:Sex:Smoker` информативность модели не изменится --- поочередно удалим. Почему лучше делать это поочередно? Так как при удалении предиктора из модели статистики других предикторов могут измениться:

```{r}
model5.2 <- update(model5.1, . ~ . -Age:Sex:Smoker)
drop1(model5.2, test = "F")
```

```{r}
model5.3 <- update(model5.2, . ~ . -Age:Height:Smoker)
drop1(model5.3, test = "F")
```

И вот мы добрались до того момента, когда исключение любого из взаиможействий значимо ухудшит модель. То есть мы максимально упростили модель, насколько это возможно. Итоговая модель выглядит так:

```{r}
summary(model5.3)
```

Почему в неё остались незначимые предикторы? Так как оказались значимы взаимодействия, а включить в модель взаиможействия без самого предиктора невозможно.

Конечно, можно делать это не руками, а автоматически, но это уже относится к сфере машинного обучения. При этом не проводится содержательным анализ того, что происходит --- пока же вы руками исключаете предикторы друг за другом, можно успеть подумать, что складывается в модели с содержательной точки зрения и почему.



[^1]: Будем говорить о влиянии, помня о тех оговорках, которые мы обозначили в предыдущей главе --- регрессия per se не показывает причинно-следственной связи.

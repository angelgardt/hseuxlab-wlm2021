# Обобщенные линейные модели. Логистическая регрессия {#logistic}

## Ограничения общих линейных моделей

Модели, которые мы изучали на предыдущих занятиях носят название **общих линейных моделей (general linear models)**. Они достаточно просты и удобны в большинстве случаев, однако имеют ряд важных ограничений.

Вспомним, как выглядит уравнение такой модели:
$$y = \beta_0 + \beta_1 x_1 + \dots + \beta_p x_p + \varepsilon$$

Предикторы в такой модели могут быть как дискретными, так и непрерывными.

Важнейшим допущением / требованием этой модели является распределение ошибки:
$$\varepsilon \thicksim \mathcal{N}(0, \, \sigma)$$

Поскольку ошибка модели должна быть распределена нормально, а моделируется среднее значение, то можно сформулировать более общее допущение / требование:
$$y_i \thicksim \mathcal{N}(\mu_i, \, \sigma)$$

Таким образом, общие линейные модели позволяют моделировать зависимости только для нормально-распределенных величин. Если же отклик модели (он же зависимая переменная) подчиняется другому распределению, эти модели не годятся.

Однако нам на помощь приходят **обобщенные линейные модели (generalized linear models)**, которые позволяют моделировать зависимости величин, подчиняющихся не только нормальному распределению, но и многим другим.

Мы познакомимся с общей логикой построения GLM, подробно рассмотрев один из вариантов таких моделей, а именно **биномиальную регрессию**.



## Биномиальное распределение

[Биномальное распределение](https://ru.wikipedia.org/wiki/Биномиальное_распределение) описывает вероятность выпадения определенного числа «успехов» в серии [испытаний Бернулли](https://ru.wikipedia.org/wiki/Схема_Бернулли).

Здесь можно [поиграться](https://angelgardt.shinyapps.io/binomial_app).



## Бинарные переменные

Эти переменные достаточно широко распространены как повседневности, так и в науке. Блюдо вкусное или невкусное, команда выиграла или проиграла, пациент в результате медицинских манипуляций выжил или умер, в ходе эксперимента была выбрана какая-то опция или нет, сдал студент экзамен или не сдал --- и т.д.

Эти события могут быть связаны с раздичными предикторами, и такую взаимосвязь можно описать с помощью регрессионных моделей.



## HR-данные

[Тут]() есть датасет про мидий. Это экспериментальные данные, которые были собраны, чтобы понять, различают ли морские звезды два вида мидий.

Ценные промысловые моллюски, атлантические мидии, обитают в Белом море, но недавно туда вселились мидии другого вида --- тихоокеанские. Вселенцы имеют меньшую промысловую значимость, и важно понять, что регулирует их численность. Наиболее значимый фактор --- морские звезды, которые ими питаются.

Грузим данные.
```{r}
library(tidyverse)
library(car)
theme_set(theme_bw())
```
```{r, cache=TRUE}
mussels <- read_csv('https://raw.githubusercontent.com/angelgardt/hseuxlab-andan/master/aster_mussel.csv')
```

Смотрим их структуру.
```{r, cache=TRUE}
str(mussels)
```

У нас есть следуюшие переменные:

* `Outcome` --- съели мидию или нет
* `Sp` --- вид мидий (`Ed` --- корреной вид, `Tr` --- вселенец)
* `L` --- длина мидий (мм)
* `Box` --- номер контейнера, в котором прододился эксперимент

Поизучаем и подправим данные.
```{r, cache=TRUE}
colSums(is.na(mussels))
table(mussels$Box)
```

Подправим данные. Сделаем факторными переменные, которые должны быть таковыми, и перекодируем `Outcome`.

```{r}
mussels %>% mutate(Box = as_factor(Box),
                   Sp = as_factor(Sp),
                   Out = ifelse(mussels$Outcome == 'eaten', 1, 0)) -> mussels
```

Перекодирование нужно для того, чтобы алгоритм подборки модели корректно отработал.



# Задача построения линейной модели

Чтобы ответить на вопрос, различают ли звезды разные виды мидий, нам нужно построить модель, описывающую связь `Outcome` (=`Out`) с предикторами `Sp` и `L`.

Попробуем обычную линейную регрессию.
```{r}
fit0 <- lm(Out ~ Sp * L, mussels)
summary(fit0)
```

```{r}
mussels %>% ggplot(aes(L, Out, color = Sp)) +
  geom_point(size=2, alpha=.5) +
  geom_line(aes(y = fit0$fitted.values))
```

Но получается что-то странное...

Во-первых, непонятно, какая величина отложена на оси *y*. Во-вторых, предсказания модели выходят за границы допустимых значений (модель предсказывает отрицательные значения). Поэтому простая линейная модель нам не подходит. Надо искать что-то еще.



# Логистическая кривая
Собственно бинарные переменные неудобны для работы, поэтому надо найти способ превратить такую дискретную бинарную шкалу в «безграничную» и непрерывную. При моделировании нулей и единиц переходят к моделированию **вероятности получения единиц**.

Сама зависимая переменная в зависимости от предиктора распределена примерно так:
![](/Users/antonangelgardt/Desktop/IMG_0189.jpeg)

Введем новые обозначения:

* $p_i$ --- вероятность события $y_i = 1$ при данных значениях предиктора,
* $1 - p_i$ --- вероятность альтернативного события $y_i = 0$.

Получается непрерывная величина $0 \leq p_i \leq 1$.

![](/Users/antonangelgardt/Desktop/IMG_0191.jpeg)

И вроде бы как ее можно уже моделировать. Но нужно помнить, что вероятность изменяется в пределах от нуля до единицы, а прямая ничем не ограничена. Поэтому прямая --- не лучший вариант.

Такая закономерность моделируется логистической кривой.
![](/Users/antonangelgardt/Desktop/IMG_0192.jpeg)

![](/Users/antonangelgardt/R/logistic_curve.png)

Она лежит как раз в пределах от 0 до 1. Наша логистическая кривая задается уравнением
$$p_i = \frac{e^{\beta_0 + \beta_1 x_i}}{1 + e^{\beta_0 + \beta_1 x_i}}$$

Логистическую кривую мы больше никогда не увидим [печаль :(], но она используется внутри функций, которыми мы строим модель.


# Шансы и логиты
Теперь нам надо побороться с ограниченностью логистической кривой. Для этого можно перейти от вероятностей к шансам.

**Шанс (отношение шансов, odds, odds ratio)** --- это отношение вероятности успеха к вероятности неудачи. Их величина варьируется от $0$ до $+\infty$.

Уже лучше, но все еще не самый лучший вариант... Последний шаг, необходимый нам, чтобы все было хорошо, юзануть логарифм, который преобразуем шансы в логиты.

$$
\mathrm{logit}(p) = \ln\left(\frac{p_i}{1-p_i}\right)
$$

Значения логитов --- трансформированные оценки вероятностей события. Они варьируют от $-\infty$ до $+\infty$, симметричны относительно нуля, и их удобно брать в качестве зависимой переменной для построения модели. Кроме того, logit-преобразование еще и линеаризует логистическую кривую. [Доказательство.](https://angelgardt.github.io/hseuxlab-andan/logit_transformation_proof.html)

Функция, используемая для линеаризации связи между предиктором и зависимой переменной, называется **связывающей функцией (linked function)**. Функция logit-преобразоввания --- одна из нескольких связывающих функций, применяемых для анализа бинарных переменных отклика.

Итак, summary от всего, что было выше:
1. От дискретной оценки событий (0 и 1) переходим к оценке вероятностей.
2. Связь вероятностей с предиктором описывается логистической кривой.
3. Если при помощи функции связи перейти от вероятностей к логитам, то связь будет описываться прямой линией.
4. Параметры линейной модели для такой прямой можно оценить с помощью регрессионного анализа.



# Формулирование модели в математическом виде

**GLM с биномиальным распределением отклика**

$$y_i \thicksim \mathrm{Bin}(n = 1, p_i)
\\
\mathbb{E}(y_i) = p_i = \frac{e^{\beta_0 + \beta_1 x_1 + \dots + \beta_p x_p}}{1 + e^{\beta_0 + \beta_1 x_1 + \dots + \beta_p x_p}}$$

Функция связи (linked function):
$$
\ln \left(\frac{p_i}{1 - p_i}\right) = \eta_i
\\
\eta_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_p x_{pi}
$$

Для перехода от логитов к вероятностям используется функция, обратная функции связи. В данном случае, логистическое преобразование:
$$
p_i = \frac{e^{\eta_i}}{1 + e^{\eta_i}}
$$



# Подбор линейной модели

Для построения модели используется функция `glm()`. Синтаксически она очень похожа на `lm()`, но у неё есть дополнительный аргумент `family`, в который необходимо указать тип регрессии, которую мы собираемся использовать. Внутри указания типа можно указать функцию связи, с использвоание которой будет строиться модель.

```{r}
fit <- glm(Out ~ Sp*L, family = binomial(link = 'logit'), mussels)
summary(fit)
```

Модель сошлась, все ровно. Вызываем функцию `summary()`, чтобы посмотреть статистики модели. Вывод функции нам уже знаком, он аналогичен тому, с которым мы встречались, однако все-таки отличается некоторыми деталями.

Во-первых, нет F-статистики. Во-вторых, нет R². Это связано с тем, что алгоритм GLM не работает с дисперсией и суммой квадратов --- модель подбирается **методом максимального правдоподобия**. Данный метод позволяет учесть разную форму распределений, которые мы можем моделировать с использованием GLM.



# Метод максимального правдоподобия

**Правдоподобие (likelihood)** --- способ измерить соответствие имеющихся данных тому, что можно получить при определенных значениях параметров модели. Оно представляет собой произведение вероятностей получения каждой из точек данных:

$$
L(\theta | \mathrm{data}) = \prod_{i=1}^n f(\mathrm{data}|\theta),
$$
где $f(\mathrm{data}|\theta)$ --- функция распределения с параметрами $\theta$.

Параметры модели должны максимизировать значения [логарифма] правдоподобия, т.е.
$$
L(\theta | \mathrm{data}) \rightarrow \max
$$
Однако для упрощения вычислений используют логарифмы правдоподобий (loglikelihood) и максимизируют их
$$
\ln L(\theta | \mathrm{data}) \rightarrow \max
$$
Аналитически такие задачи решаются редко, чаще используются методы численной оптимизации.



# Анализируем модель
Итак, продолжим изучать аутпут функции `summary()`. Обратимся к ней еще раз.
```{r}
summary(fit)
```

Мы видим знакомую нам табличку с оценками коэффициентов. В нашем случае есть intercept, которые содержит один из уровеней категориального предиктора (`SpTr`), второй уровень категориального предиктора (`SpEd`) и количественный предиктор на каждом из двух уровней (`L` и `SpEd:L`). У каждого предиктора есть информация о значении коэффициента при нем (`Estimate`), значение стандартной ошибки, z-value и p-value, рассчитанный для последнего. 

Судя по аутпуту, значимым является предиктор длины только для вида-вселенца. Но так ли это?

Внимательно посмотрим на эту табличку. Раньше у нас были значения t-статистики --- теперь z-теста. К чему это может приводить?

Чтобы ответить на этот вопрос, надо разобраться, что такое этот z-тест.

![](/Users/antonangelgardt/Desktop/zv.png)

Значение z-value является результатом подсчета **теста Вальда** и позволяет оценить значимость коэффициента модели. Расчет статистики похож на подсчет t-теста, только распределение данной статистики *ассимптотически* стремиться к нормальному (отсюда и *z*). Ассимптотика приводит к тому, что опеределение значимости коэффициентов на маленьких выборках будет неточным.

$$
H_0: \beta_k = 0 \\
H_1: \beta_k \neq 0 \\
\\
z = \frac{b_k - \beta_k}{SE_{b_k}} = \frac{b_k}{SE_{b_k}} \thicksim \mathcal{N}(0,\,1)
$$

Поскольку значения теста Вальда нам не могут обеспечить достаточной точности оценки коэффициентов, нам необходимы другие метрики оценки значимости коэффициентов.



# Анализ девиансы и упрощение модели
Для получения более точных оценок необходимо поработать в логарифмами правдоподобий. Вообще логарифмы правдоподобий используются в GLM много для чего:

* для описания качества подгонки модели;
* для тестирования значимости подобранной модели в целом;
* для тестирования значимости отдельных предикторов;
* для отбора моделей.

Дла понимания механики анализа нам потребуется несколько полезных абстракций:

* **Насыщенная модель (saturated model)** --- каждое уникальное наблюдение (сочетание предикторов) описывается одним из $n$ параметров.

$$
\ln L_{\mathrm{sat}} = 0 \\
\mathrm{df}_{\mathrm{sat}} = n - p_{\mathrm{sat}} = n - n = 0
$$

* **Нулевая модель (null model)** --- все наблюдения описываются одним параметром (средним значением)

$$
\hat y_i = \beta_0 \\
\ln L_{\mathrm{null}} \neq 0 \rightarrow -\infty \\
\mathrm{df}_{\mathrm{null}} = n - p_{\mathrm{null}} = n - 1
$$

Наша **реальная (предложенная) модель**, которую мы подбираем, будет находится где-то между насыщенной и нулевой моделью (вернее, не сама модель, а значение логарифма её правдоподобия).

$$
\hat y_i = \beta_0 + \beta_1 x_{1i} + \dots + \beta_p x_{pi} \\
\ln L_{\mathrm{model}} \neq 0 \\
\mathrm{df}_\mathrm{model} = n - p_\mathrm{model}
$$
![](/Users/antonangelgardt/Desktop/IMG_0193.jpeg)

**Девианса** является мерой различия правдоподобий двух моделей (оценка разницы логарифмов правдоподобий). [см. рисунок выше]

**Остаточная девианса**
$$
d_\mathrm{resid} = 2\big(\ln L_\mathrm{sat} - \ln L_\mathrm{model} \big) = -2 \ln(L_\mathrm{model})
$$

**Нулевая девианса**
$$
d_\mathrm{null} = 2 \big(\ln L_\mathrm{sat} - \ln L_\mathrm{null}\big) = -2 \ln(L_\mathrm{null})
$$

Сравнение нулевой и остаточной девианс позволяет судить о статистической значимости модели в целом. Такое сравнение проводится с помощью **теста отношения правдоподобий (likelihood ratio test, LRT)**.

$$
d_\mathrm{null} - d_\mathrm{resid} = -2 ( \ln L_\mathrm{null} - \ln L_\mathrm{model} ) = 2 (\ln L_\mathrm{model} - \ln L_\mathrm{null}) = 2 \ln \left( \frac{L_\mathrm{model}}{L_\mathrm{null}} \right) \\
\mathrm{LRT} = 2 \ln \left( \frac{L_\mathrm{M_1}}{L_\mathrm{M_2}} \right) = 2 (\ln L_\mathrm{M_1} - L_\mathrm{M_2}),
$$
где $M_1,\, M_2$ --- вложенные модели ($M_1$ --- более полная, $M_2$ --- уменьшенная),
$L_\mathrm{M_1}, \, L_\mathrm{M_2}$ --- правдоподобия.

Распределенеи разницы логарифмов правдоподобий аппроксиммируется распределением $\chi^2$ с числом степеней свободы $\mathrm{df} = \mathrm{df}_\mathrm{M_2} - \mathrm{df}_\mathrm{M_1}$.

LRT для тестирования значимости модели в целом:

$$
\mathrm{LRT} = 2 \ln \left( \frac{L_\mathrm{model}}{L_\mathrm{null}} \right) = 2 (\ln L_\mathrm{model} - \ln L_{\mathrm{null}}) = d_\mathrm{null} - d_\mathrm{resid} \\
\mathrm{df} = p_\mathrm{model} - 1
$$

LRT для тестирования значимости отдельных предикторов:

$$
\mathrm{LRT} = 2 \ln \left( \frac{L_\mathrm{model}}{L_\mathrm{reduced}} \right) = 2 (\ln L_\mathrm{model} - \ln L_\mathrm{reduced}) \\
\mathrm{df} = p_\mathrm{model} - p_\mathrm{reduced}
$$

Реализуем изученное в `R`.

Протестируем значимость модели в целом. Для этого неоходимо создать нулевую модель с одним предиктором --- среднее --- и передать эту модель и тестируемую в функцию `anova()` с указанием параметра `test = 'Chi'`. Получим нужным нам аутпут:

```{r}
fit_null <- glm(Out ~ 1, family = binomial(link = 'logit'), mussels)
anova(fit_null, fit, test = 'Chi')
```

Как можно увидеть, наша модель получилась статистически значима.

Для тестирования значимости отдельных предикторов можно (и лучше) использовать функцию `Anova()` из пакета `car`.

```{r}
Anova(fit)
```

Мы получаем табличку, похожую на результат работы `summary()`, но с другими статистиками.

Аналогичный результат можно получить, используя базовую функцию `drop1()`, однако аутпут будет несколько отличаться. Данная функция тестирует значимость предикторов последовательно и позволяет постепенно упрощать модель.

```{r}
drop1(fit, test = 'Chi')
```

Например, в нашем случае взаимодействие факторов оказалось статистически незначимым. Следовательно, его можно удалить из модели. Обновим имеющуюся модель в помощью функции `update()` и запишем в новый объект.

```{r}
fit1 <- update(fit, .~. -Sp:L)
drop1(fit1, test = 'Chi')
```

Повторное тестирование модели показывает, что оба оставшихся предиктора статистически значимы --- больше модель упростить нельзя. Таким образом, мы получили нашу финальную модель.

На прошлом занятии мы упоминали **[информационный критерий Акаике](https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D1%8B%D0%B9_%D0%BA%D1%80%D0%B8%D1%82%D0%B5%D1%80%D0%B8%D0%B9_%D0%90%D0%BA%D0%B0%D0%B8%D0%BA%D0%B5) (Akaike information criterion)**, позволяющий сделать выбор из нескольких моделей. Его можно использовать и в нашем случае как дополнительную проверку.

```{r}
AIC(fit, fit1)
```

С удалением взаимодействия из модели AIC снизился, что позволяет сделать вывод о том, что модель без взаимодействия лучше модели, включающей его.



# Интерпретация коэффициентов модели

Еще раз позовем summary модели:
```{r}
summary(fit1)
```

Что значат эти непонятные коэффициенты при предикторах? В общих линейных моделях все ясно как белый день, а тут какая-то дичь...

Посмотрим на математическую запись нашей модели:

$$
\mathrm{Out}_i \thicksim \mathrm{Bin}(n = 1, \, p_i) \\
\mathbb{E}(\mathrm{Out}_i) = p_i \\
\ln \left(\frac{p_i}{1 - p_i}\right) = \eta_i \\
\eta_i = 1.469 - 1.070 \, \mathrm{Sp}_{\mathrm{Ed} \, i} - 0.113 \, \mathrm L_{i}
$$


Наша зависимая переменная --- логарифм отношения шансов. От этого и будем толкаться.

Тогда,

* $b_0$, интерсепт, показывает *логарифм отношения шансов для базового уровня* (`Tr`) дискретного предиктора (`Sp`);
* $b_1$ показывает, *на сколько единиц изменяется логарифм отношения шансов* для данного уровня (`Ed`) дискретного предиктора (`Sp`) по сравнению с базовым уровнем (`Tr`);
* $b_2$ показывает, *на сколько единиц изменится логарифм отношения шансов* при изменении значения предиктора (`L`) на единицу.

Корректно, но непонятно... Чтобы разобраться лучше, немного потупим в алгебру.


### Немного алгебры для понимания сути коэффициентов

Пусть у нас есть модель с одним непрервным предиктором. Тогда она будет записываться так:

$$
\eta = b_0 + b_1 x
$$

В терминах логитов её можно записать следующим образом:

$$
\eta = \ln \left( \frac{p}{1 - p} \right) = \ln (\mathrm{odds})
$$

Как изменится предсказание модели при изменении предиктора на единицу?


$$
\eta_{x+1} - \eta_x = \ln (\mathrm{odds}_{x+1}) - \ln (\mathrm{odds}_{x}) = 
\ln \left( \frac{\mathrm{odds}_{x+1}}{\mathrm{odds}_x} \right)
$$
$$
\eta_{x+1} - \eta_x = b_0 + b_1 (x + 1) - b_0 - b_1x = b_1
$$
$$
\ln \left( \frac{\mathrm{odds}_{x+1}}{\mathrm{odds}_{x}} \right) = b_1
$$
$$
\frac{\mathrm{odds}_{x+1}}{\mathrm{odds}_{x}} = e^{b_1}
$$


Таким образом, $e^{b_1}$ показывает, во сколько раз изменится отношение шансов при увеличении предиктора на единицу. Для дискретных предикторов $e^{b_1}$ покажет, во сколько раз различается отношение шансов для данного уровня предиктора по сравнению с базовым.

***

![](/Users/antonangelgardt/Desktop/geom_interpret.png)

***

Теперь мы можем тракровать полученные коэффициенты нормальным языком.

$$
\eta_i = 1.469 - 1.070 \, \mathrm{Sp}_{\mathrm{Ed} \, i} - 0.113 \, \mathrm L_{i}
$$

При увеличении длины тела мидии на 1 мм отношения шансов быть съеденной увеличатся в $e^{−0.113} = 0.893$ раза. То есть мидия, имеющая больший размер, имеет меньше шансов быть съеденной (больше шансов на выживание).

Отношение шансов быть съеденной у мидии, относящейся к группе `Ed` дискретного фактора `Sp`, отличается в $e^{-1.07} = 0.343$ раза от аналогичной характеристики мидии, относящейся к базовому уровню (`Tr`). То есть вероятность выжить у мидии из группы `Ed` больше, чем у мидии из группы `Tr`.



# Предсказательная сила модели
Проверим, насколько эффективна наша модель для предсказаний. Воспользуется функцией `predict()` и создадим новую колонку с предсказанными вероятностями.

```{r}
mussels %>% mutate(predicted = predict(fit1, type = 'response')) -> mussels
```

Далее нам необходимо выбрать критерий --- пороговое значение вероятность ---- после которого мы будем решать, что данную мидию съели. Выберем очень либеральный критерий. Результаты заносим в новый столбец.

```{r}
mussels %>% mutate(out_pred = ifelse(mussels$predicted > .6, 'eaten_p', 'not_eaten_p')) -> mussels
```

Теперь построим **confusion matrix** и посмотрим на качество предсказаний:

```{r}
table(mussels$Outcome, mussels$out_pred)
```

Качество предсказаний у нас мягко говоря нутакое. Модель предсказывает в прицнипе очень маленькую вероятность того, что мидию съедят. То есть несмотря на то, что для научных целей изучения и анализа закономерностей она хороша, для каких-либо предсказаний она не годится.


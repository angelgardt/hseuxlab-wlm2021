[["cluster.html", "25 Кластерный анализ 25.1 Кластерный анализ 25.2 Геометрическая интерпретация задачи кластеризации 25.3 Расстояние между кластерами 25.4 Иерархическая кластеризация 25.5 k-means (метод k-средних) 25.6 Интерпретация результатов кластерного анализа 25.7 Метрики качества кластеризации 25.8 Нечёткий кластерный анализ", " 25 Кластерный анализ К этому моменту мы с вами уже научились решать много разных аналитических задач, которые глобально можно объединить в две группы: задачи регрессии — когда нам необходимо выяснить влияние определенных факторов на количественную переменную, а также предсказать её значение (простая и множественная линейная регрессия, регуляризованная регрессия, пуассоновкая регрессия, дисперсионный анализ). задачи классификации — когда нам необходимо определить, к какому классу относиться объект на основе признаков, которыми он обладает (биномиальная регрессия). Но нам часто бывает нужно определить, какие наблюдения наиболее похожи друг на друга, то есть разбить их на группы, при условии что нам неизвестно, какие это будут группы. Эта задача носит название кластеризации. Ей мы и посвятим эту главу. 25.1 Кластерный анализ Задача кластерного анализа — разбиение набора объектов на группы, при этом попутно определяется число этих групп. Группы, на которые разбивается выборка, называются кластеры. 25.2 Геометрическая интерпретация задачи кластеризации Напомним себе, что компьютер умеет работать только с числами упорядоченое множество объектов одного типа есть вектор каждый вектор мы [когда-то давно] договаривались начинать из начала координат, а значит, может описать его только координатами конца Теперь посмотрим на данные. Как мы знаем, столбцы — это переменные, или характеристики объектов. Строки — это сами объекты. Любой объект мы можем описать числовым вектором, где числа задают значение характеристик объектов. Если это количественные характеристики, то тут всё понятно — это воистину числа. А если характеристики качественные? Никаких проблем — мы их перекодируем в числа! Если это бинарные переменные (например, пол или ступень обучения «бакалавр»/«магистр»), то одну категорию обозначим 0, другую — 1. Если категорий больше, то у нас просто будет больше чисел-индикаторов. Итого, каждое наблюдение описывается числовым вектором, а следовательно, и некоторой точкой в пространстве. Каково это пространство? Оси — это переменные, то есть характеристики объектов. Измерений в этом признаковом пространстве столько, сколько переменных в нашем датасете. Наша задача — объединить похожие наблюдения в группы. А какие наблюдения являются похожими? Логично допустить, что те, которые обдалают схожими характеристиками. Если характеристики объектов схожи, то в признаковом пространстве они будут располагаться близко друг к другу. Итого, summary геометрической задачи: каждый из \\(n\\) рассматриваемых объектов — это точка в некотором \\(p\\)-мерной признаковом пространстве; похожие объекты будут располагаться «близко» друг с другу; различающиеся объекты будут располагаться «далеко» друг от друга; скопления точек — это искомые кластеры. 25.2.1 Проблема кластеризации Посмотрим на простейший вариант — двухмерное признаковое пространство. Пусть у нас есть некоторые два признака, которые будут задавать два соответствующих измерения, и некоторое количество точек, которые располагаются примерно так: КАРТИНКА Сколько здесь кластеров? Кто-то скажет, что их три: КАРТИНКА Кто-то скажет, что их четыре: КАРТИНКА Кто-то скажет, что их всё же три, но выглядят они по-другому: КАРТИНКА Что мы здесь наблюдаем? Проблему. Разные методы кластеризации могут давать разные результаты. Какой из них верный? Неясно… так как истинная группировка данных нам неизвестна. Но мы будем пытаться как-то выживать в ситуаций такой неопределённости. 25.2.2 Расстояние между объектами Обратим внимание на следующий важный момент. Мы оперируем терминами «близко» и «далеко» — но как мы определаем расстояние между объектами? Рассмотрим самые популярные и важные для нас варианты. 25.2.2.1 Евклидово расстояние С одним из них мы знакомы ещё со школы — это евклидово расстояние между точками. По смыслу это длина отрезка, соединяющего две точки. Оно определяется как корень из суммы квадратов покоординатных разностей. Пусть у нас есть две точки — \\((x_1, x_2, \\dots, x_p)\\) и \\((y_1, y_2, \\dots, y_p)\\). Евклидово расстояние будет определяться по формуле: \\[ d_{\\text{Eucl}, XY} = \\sqrt{\\sum_{j=1}^p (x_j - y_j)^2} \\] Иногда также используется квадрат евклидова расстояния1. 25.2.2.2 Манхэттеновское расстояние Оно же блок-расстояние или расстояние таксиста. Представим славный российский город Санкт-Петербург, где, как известно, улицы организованы вот так: КАРТИНКА Какое расстояние проедет таксист из точки \\(A\\) в точки \\(B\\)? КАРТИНКА Правильно, вот такое2: КАРТИНКА Схожая логика может быть использована и при расчёте расстояния между точками3: КАРТИНКА Математически это будет определено так: \\[ d_{\\text{Manh}, XY} = \\sum_{j=1}^p |x_j - y_j| \\] 25.2.2.3 Евклид vs Манхэттен Когда какое расстояние выбирать? Здесь два важных момента. Первый — математический. Как и в случае с дисперсией, мы возводим покоординатные разности в квадрат. Если переменные измерены в различных единицах, то вклад одной из них в суммарное расстояние может быть значительно выше, чем других. По этой причине необходимо принять решение: является ли большая разница значений по одной из переменных достаточным основанием для отнесения наблюдений к различным кластерам? Если да, то можно использовать такое расстояние, если нет, то либо необходима стандартизация переменных, либо использование расстояния Манхэттен. Второй — измерительный. Если переменные, по которым вы кластеризуете наблюдения, непрерывные, то можно использовать евклидово расстояние. Если переменные дискретные, то более логичным вариантом будет манхэттеновское расстояние. 25.2.2.4 Другие виды расстояний Отметим, что есть и другие виды расстояний, когда мы работает не с числовыми объектами. Например, мы можем пытаться кластеризовать слова — задача непростая, но её можно пытаться решить. Например, с помощью расстояний Хэмминга или Левенштейна. Для более специфичных объектов могут понадобиться и более изощрённые метрики расстояний. Да и вообще «никакое время, потраченное на раздумья, какое расстояние выбрать, не будет потрачено зря»4. 25.2.3 Проблема операционализации расстояния Но вообще нам надо здесь поговорить ещё вот о чём: как вообще мы определяем, что есть расстояние между объектами? То есть как мы его операционализируем? Например, мы хотим кластеризовать наших испытуемых на «эффективных решателей задачи» и «неэффективных решателей задачи». По каким параметрам мы это будем делать? Как вариант — время решения и число ошибок в ходе решения. А как мы будем замерять эти переменные? Первую, видимо, в непрерывной шкале, вторую — в дискретной. Далее будем решать вопрос о выборе конкретной метрики расстояния. Хорошо, а как нам кластеризовать менеджеров на «хороших», «плохих» и «средненьких продажников»? Можем использовать разные подходы: оценку 360, показатели KPI и т. д. А как нам определять расстояние между сайтами? По каким показателям? Здесь вариантов ещё больше и всё зависит от конкретной аналитической задачи. Это всё о чем? О том, что операционализировать расстояние не так-то просто и для разных задач расстояние между одними и теми же объектами может быть операционализировано по-разному. 25.2.4 Субъективность кластерного анализа Мы плавно подъехали к ещё одной проблеме-особенности. отбор переменных 25.3 Расстояние между кластерами Хорошо, мы поговорили о том, как считать расстояние между объектами, но нам надо понять, насколько (не)похожи получившиеся группы объектов. Для этого придется считать расстояние между кластерами. Варинатов, как обычно, масса. 25.3.1 Среднее невзвешенное расстояние Среднее невзвешенное рассрояние (Average linkage clustering) определяется так: находим расстояния между всеми парами объектов двух кластеров усредняем их КАРТИНКА 25.3.2 Центроидный метод Ранее был самым популярным из-за вычислительной простоты. Определяется расстояние между центрами тяжести двух кластеров: КАРТИНКА В настоящее время используется крайне мало. 25.3.3 Метод дальнего соседа Расстояние между кластерами определяется как расстояние между наиболее удалёнными объектами кластеров: КАРТИНКА 25.3.4 Метод ближайшего соседа Расстояние между кластерами определяется как расстояние между наиболее близкими объектами кластеров: КАРТИНКА 25.3.5 Расстояние Sørensen—Dice Довольно экзотичная метрика, но может быть полезна, например, при определии расстояний между сайтами: \\[ Q = \\frac{2|A \\cap B|}{|A| + |B|} \\] Могут использоваться его модификации, например: \\[ Q = \\frac{|A \\cap B|}{|A| + |B| - |A \\cap B|} \\] 25.4 Иерархическая кластеризация 25.4.1 Алгоритм иерархического кластерного анализа 25.4.2 Иерархическая кластеризация в R 25.5 k-means (метод k-средних) 25.5.1 Алгоритм метода k-средних 25.5.2 Типы кластеров 25.6 Интерпретация результатов кластерного анализа 25.7 Метрики качества кластеризации 25.7.1 Внешние метрики 25.7.2 Внутренние метрики 25.8 Нечёткий кластерный анализ Хотя математически эта метрика не является расстоянием, так как может нарушаться неравенство треугольника. Однако для задач кластерного анализа это не имеет большого значения.↩︎ Заметьте, что в количестве кварталов все расстояния равны.↩︎ Заметьте, что длина всех траекторий также будет одинакова.↩︎ Вадим Аббакумов, лекция в Computer Science Center↩︎ "]]

[["correlation.html", "16 Корреляционный анализ 16.1 Ковариация 16.2 Корреляция", " 16 Корреляционный анализ До этого момента мы рассматривали только отдельные переменные и их характерики, однако в практике мы редко работаем только с одной переменной. Как правило, у нас есть многомерное пространство признаков, и нас интересуют взаимосвязи между ними. 16.1 Ковариация Мы хотим описать имеющиеся взаимосвязи как можно проще и опираясь на то, что у нас уже есть. Мы говорили, что дисперсия, или вариация, заключает в себе информацию об изменчивости признака. Если мы хотим исследовать взаимосвязь между признаками, то логично будет посмотреть, как изменяется один из признаков при изменении другого — иначе говоря, рассчитать совместную изменчивость признаков, или ко-вариацию (covariance). Как мы её будем считать? Подумаем графически. Расположим две переменные на осях и сопоставим каждому имеющемуся наблюдению точку на плоскости. КАРТИНКА Отметим средние значения по обеим переменным. КАРТИНКА Заметим, что если наши наблюдения по переменной \\(x_1\\) отклоняются в большую сторону, то они отклоняются в большую сторону и по переменной \\(x_2\\). Аналогично, если они будут отклоняться в меньшую сторону по \\(x_1\\), то в меньшую же сторону они будут отклоняться и по \\(x_2\\). КАРТИНКА Получается, мы можем на основании согласованности отклонений уже заключить о направлении связи. Произведение отклонений по обеим величинам будет положительно, если отклонения сонаправленны. Запишем это математически. \\[ (\\bar x_1 - x_{i1}) (\\bar x_2 - x_{i2}) &gt; 0 \\Leftarrow \\big( (\\bar x_1 - x_{i1}) &gt; 0 \\wedge (\\bar x_2 - x_{i2}) &gt; 0 \\big) \\vee \\big( (\\bar x_1 - x_{i1}) &lt; 0 \\wedge (\\bar x_2 - x_{i2}) &lt; 0 \\big) \\] Соответственно, если отклонения будут направлены в разные стороны, из произведение будет отрицательным. Ну, осталось только понять, как совместные отклонения организованы в среднем — это и будет ковариацией двух величин: \\[ \\mathrm{cov}(X_1, X_2) = \\frac{1}{n-1} \\sum_{i=1}^n (\\bar x_1 - x_{i1}) (\\bar x_2 - x_{i2}) \\] Что такое ковариация величины самой с собой (\\(\\mathrm{cov}(X_1, X_1)\\))? Докажите через выведение формулы. Важно отметить, что ковариация улавливается только линейную составляющую взаимосвязи между признаками, поэтому если \\(\\mathrm{cov}(X_1,X_2) = 0\\), то мы можем сказать, что между переменными нет линейной взаимосвязи, однако это не значит, что между этими переменными нет никакой другой зависимости. У ковариации есть два важных недостатка: это размерная величина, поэтому её значение зависит от единиц измерения признаков, она зависит от дисперсий признаков, поэтому по её значению можно определить только направление связи (прямая или обратная), однако ничего нельзая сказать о силе связи. Поэтому нам нужно как-то модицифировать эту статистику, чтобы мы могли больше вытащить из её значения. 16.2 Корреляция Раз ковариация зависит от дисперсии, то можно сделать некоторые математические преобразования, чтобы привести эмпирические распределения к какому-то одному виду — сделать так, чтобы они имели одинакое математическое ожидание (среднее) и одинаковую дисперсию. С этой задачей прекрасно справляется стандартизация. Напоминаю формулу: \\[ x_i^* = \\frac{x_i - \\bar x}{s} \\] После такого преобразования математическое ожидание нашего распределения будет равно нуля, а стандартное отклонение — единице. Это избавит нас от влияния дисперсии на значение ковариации. Ковариация двух стандартно нормально распределенных величин называется корреляцией (correlation). \\[ \\mathrm{cov}(X_1^*, X_2^*) = \\frac{1}{n-1} \\sum_{i=1}^n x_{i1}^* x_{i2}^* = \\mathrm{corr}(X_1, X_2), \\] где \\(X_1^*\\) и \\(X_2^*\\) — стандартизированные величины \\(X_1\\) и \\(X_2\\) соответственно. Корреляцию можно выразить через ковариацию: \\[ \\mathrm{corr}(X_1, X_2) = \\frac{1}{n-1} \\sum_{i=1}^n \\Big( \\frac{\\bar x_1 - x_{i1}}{s_1} \\Big) \\Big( \\frac{\\bar x_2 - x_{i2}}{s_2} \\Big) = \\frac{1}{s_1 s_2} \\Big( \\frac{1}{n-1} \\sum_{i=1}^n (\\bar x_1 - x_{i1})(\\bar x_2 - x_{i2}) \\Big) = \\frac{\\mathrm{cov}(X_1, X_2)}{s_1 s_2} \\] Если внимательно всмотреться в формуле, то можно обнаружить, что корреляция это не что иное, как стандартизированное значение ковариации. Коэффициент корреляции имеет четкие пределы изменения: \\([-1; \\,1]\\). Крайнее левое значение говорит о том, что присутствует полная обратная линейная взаимосвязь, крайнее правое — что присутствует полная прямая линейная взаимосвязь. Как и ковариация, корреляция ловит только линейную составляющую связи, поэтому нулевое значение корреляци показывает, что между переменными отсутствует линейная взаимосвязь. Это всё еще не значит, что связи нет вовсе. 16.2.1 Тестирование статистической значимости коэффициента корреляции Оценку коэффициента корреляции мы получаем методом моментов, заменяя истинный момент \\(\\rho_{ij}\\) выборочным \\(r_{ij}\\): \\[ \\hat \\rho_{ij} = \\overline{\\big( (X_{ki} - \\bar X_i) (X_{kj} - \\bar X_j) \\big)} = r_{ij} \\] Если в генеральной совокупности связь между признаками отсутствует, то есть \\(\\rho_{ij} = 0\\), будет ли равен нулю \\(r_{ij}\\)? Можно с уверенностью сказать, что не будет, так как выборочный коэффициент корреляции — случайная величина. А мы помним, что вероятность принятия случайной величиной своего конкретного значения равна нулю. Тогда необходимо протестировать статистическую гипотезу: \\[ H_0: \\rho_{ij} = 0 \\; \\text{(линейной связи нет)} \\\\ H_1: \\rho_{ij} \\neq 0 \\text{(наиболее частый вариант альтернативы)} \\] Для проверки нулевой гипотезы используется следующая статистика: \\[ t = \\frac{r_{ij}}{\\sqrt{\\frac{1 - r^2_{ij}}{n-2}}} \\overset{H_0}{\\thicksim} t(\\nu = n-2) \\] Вывод о статистической значимости коэффициента корреляции делается согласно алгоритму тестировния статистических гипотез. 16.2.2 Доверительный интервал для коэффициента корреляции С построением интервальной оценки коэффциента корреляции возникают некоторые сложности. Наша задача состоит в том, чтобы определить в каких границах будет лежать значение истинного коэффициента корреляции с заданной вероятностью: \\[ \\mathrm{P} (\\rho_{ij,\\min} &lt; \\rho_{ij} &lt; \\rho_{ij,\\max}) = \\gamma \\] Нам необходимо найти статистику, закон распределения корой известен, однако ранее упомянутся статистика не подходит, так как она имеет распределение Стьюдента, когда верна нулевая гипотеза об отсутствии связи. Если же мы строим интервальную оценку, нас интересует случай наличия связи. Такую статистику искали долго, и её удалось найти, когда ввели определённое преобразование выборочного критерия корреяции — z-преобразования Фишера: \\[ z(r_{ij}) = \\frac{1}{2} \\ln \\frac{1 + r_{ij}}{1 - r_{ij}} \\thicksim \\mathrm{N}(\\bar z_{ij}, \\tfrac{1}{n-3}), \\] где \\(n\\) — объём выборки, а \\(\\bar z_{ij}\\) получается расчётом по указанной формуле после подставления точечной оценки коэффициента корреляции. Тогда интервальная оценка для величины \\(z_{ij, \\mathrm{true}}\\) приобретает такой вид: \\[ \\mathrm{P} \\Big( \\bar z_{ij} - t_\\gamma \\sqrt{\\tfrac{1}{n-3}} &lt; z_{ij, \\mathrm{true}} &lt; \\bar z_{ij} + t_\\gamma \\sqrt{\\tfrac{1}{n-3}} \\Big) = \\gamma = \\Phi(t_\\gamma) \\] Далее путём обратного преобразования получаются значения границ интервала \\((\\rho_{ij,\\min}, \\; \\rho_{ij,\\max})\\). "]]

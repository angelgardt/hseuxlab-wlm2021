# Эксплораторный факторный анализ {#efa}

\newcommand{\X}{\boldsymbol{X}}
\newcommand{\Y}{\boldsymbol{Y}}
\newcommand{\F}{\boldsymbol{F}}
\newcommand{\U}{\boldsymbol{U}}
\newcommand{\A}{\boldsymbol{A}}
\newcommand{\I}{\boldsymbol{I}}
\newcommand{\var}{\mathrm{var}}
\newcommand{\T}{\mathrm{T}}
\newcommand{\a}{\boldsymbol{a}}
\newcommand{\corr}{\mathrm{corr}}

Он же **exploratory factor analysis**.

## Математическая модель эскплораторного факторного анализа

У нас есть все та же матрица наблюдений $\X^\T = (\X_1 \, \X_2 \, \dots \X_k)$, только мы ее транспонировали для будущего удобства. Мы предполагаем, что под нашими наблюдениями спрятаны некие факторы в количестве $p$ штук, $p < k$. Их мы также можем оформить в матрицу $\F^\T=(\F_1 \, \F_2 \, \dots \F_p)$. Эти факторы объясняют имеющиеся переменные. Делают это они следующим образом:

$$
\X_i = a_{i1} \F_1 + a_{i2} \F_ 2 + \dots + a_{ip} \F_p + \U_i, \; 1≤i≤k
$$

$$
\X = \A \F + \U, \\
\A = (a_{ij}),\; 1≤i≤k, \; 1≤j≤p, \\
\U^\T = (\U_1 \, \U_2 \dots \U_k)
$$ 

Здесь $U$ --- то, что не удалось объяснить факторами (остатки, уникальность, *uniqueness*).

Также выдвигаем ряд дополнительных предположения для формализации требования и упрощения вычислений:

1. $\mathbb{E} \X = 0$. Так сделать можно, так как мы все равно будем стандартизировать переменные, и математическое ожидание обратиться в ноль.
2. $\corr(\F_j, \F_t) = 0, \; ∀j\,∀t, \;j≠t, \; 1≤j≤p, \; 1≤t≤p$. То есть хотим, чтобы факторы были независимы (некоррелированы).
3. $\var(\F)=\I$. Если «истинная» дисперсия факторов будет отличаться от единицы, то разница уйдет в матрицу A.
4. $\corr(\U_i, \U_r) = 0; \;\; \corr(\U_i, \F_j) = 0,\; ∀i\,∀r\,∀j, i \neq r, \; 1≤i≤k, \; 1≤r≤k, \; 1≤j≤p$. Ну, они же все-таки уникальности.

Элементы матрицы $\A$ называются *факторными нагрузками (factor loadings)*. Элементы вектора $\U$ называются *уникальными факторами (specific variates)*.

Теперь об информативности. Аналогично PCA, возьмем в качестве меры информативности дисперсию. Мы хотим узнать, насколько хорошо факторы объясняют исходные переменные. Дисперсия переменных будет складываться из следующего:

$$
\var(\X_i) = \sum_{j=1}^p a^2_{ij} + \var(\U_i)
$$



То есть, чем больше **уникальность (uniqueness)** --- часть дисперсии переменной, объясненной уникальными факторами --- тем хуже наши факторы объясняют переменную. Что делать? Либо подобрать другую модель (изменить количество факторов), либо не брать такую переменную в факторный анализ…

Однако в факторном анализе есть еще одна интересная деталь. Мы работаем с выборочными корреляциями. По этой причине мы не можем подобрать единственно верное решение задачи факторного анализа. Матрица факторных нагрузок определена до ортогонального преобразования (по-русски, вращения). То есть у нас есть множество решений, которые отличаются друг от друга поворотом, и мы можем выбирать самое «симпатишное».

> За такую неопределенность факторный анализа часто критикуют. Да и вообще, как вы могли заметить, в факторном анализе субъективность чуть ли не на каждом шагу.


### Вращение факторов (factor rotation)

Вращать вообще-то не обязательно. Если хочется, чтобы все было похоже на PCA, то можно ввести дополнительное условие, чтобы матрица $\A^\T \text{diag}(\var(\U))\A$ быда диагональной и элементы на её главной диагонали стояли в убывающем порядке.

Но, господи, вращать это же весело!

Поэтому придумали много всяких разных вариантов вращения. Самый популярный --- *varimax.* Его идея состоит в том, чтобы найти наиболее «контрастное» решение --- чтобы значения факторов были или большими, или маленькими. Эта логика хорошо согласуется с тем, что мы хотели, чтобы каждый фактор описывал что-то своё. Попутно мы минимизируем количество переменных, которые имеют высокую нагрузку на каждый фактор. Работает *varimax* по следуюшей математической модели:

$$
\cases{
a_2 + b_ 2 \rightarrow \max \\
a + b = 1
}
$$

Коротенечко о других вращениях:

* *quartimax* --- минимизирует количество факторов, необходимых для объяснения каждой переменной
* *equamax* --- количество переменных, сильно нагружающих фактор, и количество факторов, необходимых для объяснения переменных, минимальны
* *promax* --- наклонное вращение, позволяющее коррелировать факторы



## Реализация EFA в R

Работаем с тем же датасетом:

* `brand` --- Pizza brand (class label)
* `id` --- Sample analysed
* `mois` --- Amount of water per 100 grams in the sample
* `prot` --- Amount of protein per 100 grams in the sample
* `fat` --- Amount of fat per 100 grams in the sample
* `ash` --- Amount of ash per 100 grams in the sample
* `sodium` --- Amount of sodium per 100 grams in the sample
* `carb` --- Amount of carbohydrates per 100 grams in the sample
* `cal` — Amount of calories per 100 grams in the sample

Строки из предыдущей главы:

```{r}
library(tidyverse)
theme_set(theme_bw())
pizza <- read_csv('https://raw.githubusercontent.com/angelgardt/hseuxlab-andan/master/Pizza.csv')
pizza %>% mutate(brand = as_factor(brand),
             id = as_factor(id)) %>% 
  select(-brand, -id) -> pizza2 # убираем номинативные переменные
```

Для факторного анализа есть функция `factanal()`. Она ждет от нас данные и количество факторов.

```{r error=TRUE}
factanal(pizza2, factors = 5)
```

Опа, попросили слишком много факторов. Действительно, если математические ограничения на количество факторов, оно завязано на число имеющихся переменных. Но нам не надо об этом париться, R за нас сам все проверит и посчитает.

Попробуем 4:

```{r error=TRUE}
factanal(pizza2, factors = 4)
```

Тоже много. Давайте 3:

```{r}
fan <- factanal(pizza2, factors = 3, scores = 'regression') #укажем scores, чтобы сохранились сами значения факторов
fan
```

О, норм, работает.

Итак, что видно в аутпуте: уникальности, матрица факторных нагрузок, доля объясненной дисперсии и даже какой-то статистический тест с p-value.

Анализируем уникальности: больших значений нет, значит все наши переменные хорошо объясняются факторами.

Смотрим на таблицу факторных нагрузок. Так получается, что она еще и автоматически корреляционная матрица переменных с факторами. Вот почему:

$$
\corr(\X_i,\F_j) = \corr(a_{i1} \F_1 + a_{i2} \F_2 + \dots + a_{ip} \F_p + \U_i, \F_j) = \\ = \corr(a_{i1} \F_1, Fj) + \corr(a_{i2} 
\F_2, \F_j) + \dots + \corr(a_{ij} \F_j, \F_j) + \dots + \corr(a_{ip} \F_p, \F_j)+ \corr(\U_i, \F_j) = \\ = a_{ij} \corr(\F_j, \F_j) = a_{ij} \mathrm{var}(\F_j) = a_{ij}
$$

Таким образом, мы интерпретируем факторы так же, как мы интерпретировали главные компоненты — на основании корреляций с исходными переменными. Как бы вы проинтерпретировали получившиеся три фактора?

Можно подобрать модель с двумя факторами:

```{r}
factanal(pizza2, factors = 2)
```

Как теперь их можно интерпретировать? Отличается ли результат от PCA?

Из результатов факторного анализа можно отдельно доставать всякие штуки:

```{r}
fan$loadings # факторные нагрузки aka матрица корреляций переменных с факторами
```

```{r}
fan$correlation # корреляционная матрица исходных переменных
```

```{r}
fan$uniquenesses # уникальности
```

```{r}
fan$factors # количество факторов
```

```{r}
fan$dof # число степеней сводобы
```

```{r}
fan$rotmat # rotation matrix (кстати, мы по умолчанию применили varimax)
```

```{r}
fan$n.obs # число наблюдений
```

```{r}
fan$scores # значения факторов
```

Ну, и напоследок, визуализируем связь номинативного предиктора с получившимися факторами.

```{r}
library(plotly)
```

```{r}
plot_ly(x = fan$scores[,1], y = fan$scores[,2], z = fan$scores[,3], color = pizza$brand,
                type = 'scatter3d', mode = 'markers') %>% 
  layout(scene = list(
    xaxis = list(title = 'Factor 1'),
    yaxis = list(title = 'Factor 2'),
    zaxis = list(title = 'Factor 3')
  ))
```

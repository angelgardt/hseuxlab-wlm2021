# Кластерный анализ {#cluster}

К этому моменту мы с вами уже научились решать много разных аналитических задач, которые глобально можно объединить в две группы:

* *задачи регрессии* --- когда нам необходимо выяснить влияние определенных факторов на количественную переменную, а также предсказать её значение (простая и множественная линейная регрессия, регуляризованная регрессия, пуассоновкая регрессия, дисперсионный анализ).
* *задачи классификации* --- когда нам необходимо определить, к какому классу относиться объект на основе признаков, которыми он обладает (биномиальная регрессия).

Но нам часто бывает нужно определить, какие наблюдения наиболее похожи друг на друга, то есть разбить их на группы, при условии что нам неизвестно, какие это будут группы. Эта задача носит название *кластеризации*. Ей мы и посвятим эту главу.


## Кластерный анализ

*Задача кластерного анализа* --- разбиение набора объектов на группы, при этом попутно определяется число этих групп. Группы, на которые разбивается выборка, называются *кластеры*.


## Геометрическая интерпретация задачи кластеризации

Напомним себе, что

1) компьютер умеет работать только с *числами*
2) упорядоченое множество объектов одного типа есть *вектор*
3) каждый вектор мы [когда-то давно] договаривались начинать из начала координат, а значит, может описать его только *координатами конца*

Теперь посмотрим на данные. Как мы знаем, столбцы --- это переменные, или характеристики объектов. Строки --- это сами объекты. Любой объект мы можем описать числовым вектором, где числа задают значение характеристик объектов. Если это количественные характеристики, то тут всё понятно --- это воистину числа. А если характеристики качественные? Никаких проблем --- мы их перекодируем в числа! Если это бинарные переменные (например, пол или ступень обучения «бакалавр»/«магистр»), то одну категорию обозначим `0`, другую --- `1`. Если категорий больше, то у нас просто будет больше чисел-индикаторов. Итого, *каждое наблюдение описывается числовым вектором*, а следовательно, и *некоторой точкой в пространстве*.

Каково это пространство? Оси --- это переменные, то есть характеристики объектов. Измерений в этом *признаковом пространстве* столько, сколько переменных в нашем датасете.

Наша задача --- объединить похожие наблюдения в группы. А какие наблюдения являются похожими? Логично допустить, что те, которые обдалают схожими характеристиками. Если характеристики объектов схожи, то в признаковом пространстве они будут располагаться *близко* друг к другу.

Итого, summary геометрической задачи:

* каждый из $n$ рассматриваемых объектов --- это точка в некотором $p$-мерной признаковом пространстве;
* похожие объекты будут располагаться «близко» друг с другу;
* различающиеся объекты будут располагаться «далеко» друг от друга;
* скопления точек --- это искомые кластеры.


### Проблема кластеризации

Посмотрим на простейший вариант --- двухмерное признаковое пространство. Пусть у нас есть некоторые два признака, которые будут задавать два соответствующих измерения, и некоторое количество точек, которые располагаются примерно так:

КАРТИНКА

Сколько здесь кластеров? Кто-то скажет, что их три:

КАРТИНКА

Кто-то скажет, что их четыре:

КАРТИНКА

Кто-то скажет, что их всё же три, но выглядят они по-другому:

КАРТИНКА

Что мы здесь наблюдаем? Проблему. **Разные методы кластеризации могут давать разные результаты.** Какой из них верный? Неясно… так как истинная группировка данных нам неизвестна. Но мы будем пытаться как-то выживать в ситуаций такой неопределённости.


### Расстояние между объектами

Обратим внимание на следующий важный момент. Мы оперируем терминами «близко» и «далеко» --- но как мы определаем расстояние между объектами? Рассмотрим самые популярные и важные для нас варианты.

#### Евклидово расстояние

С одним из них мы знакомы ещё со школы --- это **евклидово расстояние между точками**. По смыслу это длина отрезка, соединяющего две точки. Оно определяется как корень из суммы квадратов покоординатных разностей. Пусть у нас есть две точки --- $(x_1, x_2, \dots, x_p)$ и $(y_1, y_2, \dots, y_p)$. Евклидово расстояние будет определяться по формуле:

$$
d_{\text{Eucl}, XY} = \sqrt{\sum_{j=1}^p (x_j - y_j)^2}
$$

Иногда также используется **квадрат евклидова расстояния**[^1].

[^1]: Хотя математически эта метрика не является расстоянием, так как может нарушаться неравенство треугольника. Однако для задач кластерного анализа это не имеет большого значения.


#### Манхэттеновское расстояние

Оно же **блок-расстояние** или **расстояние таксиста**. Представим славный российский город Санкт-Петербург, где, как известно, улицы организованы вот так:

КАРТИНКА

Какое расстояние проедет таксист из точки $A$ в точки $B$?

КАРТИНКА

Правильно, вот такое[^3]:

[^3]: Заметьте, что в *количестве кварталов* все расстояния равны.

КАРТИНКА

Схожая логика может быть использована и при расчёте расстояния между точками[^2]:

[^2]: Заметьте, что длина всех траекторий также будет одинакова.

КАРТИНКА

Математически это будет определено так:

$$
d_{\text{Manh}, XY} = \sum_{j=1}^p |x_j - y_j|
$$


#### Евклид vs Манхэттен

Когда какое расстояние выбирать? Здесь два важных момента.

Первый --- *математический*. Как и в случае с дисперсией, мы возводим покоординатные разности в квадрат. Если переменные измерены в различных единицах, то вклад одной из них в суммарное расстояние может быть значительно выше, чем других. По этой причине необходимо принять решение: **является ли большая разница значений по одной из переменных достаточным основанием для отнесения наблюдений к различным кластерам?** Если да, то можно использовать такое расстояние, если нет, то либо необходима стандартизация переменных, либо использование расстояния Манхэттен.

Второй --- *измерительный*. Если переменные, по которым вы кластеризуете наблюдения, непрерывные, то можно использовать евклидово расстояние. Если переменные дискретные, то более логичным вариантом будет манхэттеновское расстояние.


#### Другие виды расстояний

Отметим, что есть и другие виды расстояний, когда мы работает не с числовыми объектами. Например, мы можем пытаться кластеризовать слова --- задача непростая, но её можно пытаться решить. Например, с помощью расстояний Хэмминга или Левенштейна. Для более специфичных объектов могут понадобиться и более изощрённые метрики расстояний. Да и вообще «никакое время, потраченное на раздумья, какое расстояние выбрать, не будет потрачено зря»[^4].

[^4]: Вадим Аббакумов, лекция в Computer Science Center


### Проблема операционализации расстояния

Но вообще нам надо здесь поговорить ещё вот о чём: как вообще мы определяем, что есть расстояние между объектами? То есть как мы его операционализируем?

Например, мы хотим кластеризовать наших испытуемых на «эффективных решателей задачи» и «неэффективных решателей задачи». По каким параметрам мы это будем делать? Как вариант --- время решения и число ошибок в ходе решения. А как мы будем замерять эти переменные? Первую, видимо, в непрерывной шкале, вторую --- в дискретной. Далее будем решать вопрос о выборе конкретной метрики расстояния.

Хорошо, а как нам кластеризовать менеджеров на «хороших», «плохих» и «средненьких продажников»? Можем использовать разные подходы: оценку 360, показатели KPI и т. д.

А как нам определять расстояние между сайтами? По каким показателям? Здесь вариантов ещё больше и всё зависит от конкретной аналитической задачи.

Это всё о чем? О том, что операционализировать расстояние не так-то просто и для разных задач расстояние между одними и теми же объектами может быть операционализировано по-разному.


### Субъективность кластерного анализа

Мы плавно подъехали к ещё одной проблеме-особенности. Какова роль аналитика в кластерном анализе? Достаточно велика:

* отбор переменных для анализа
  - какие переменные включать в анализ? все? или необходимо проводить отбор?
  - возможно наличие переменных, которые будут хорошо работать с точки зрения поиска схожих объектов, но это не то сходство, которое мы ищем
  - одни переменные могут быть косвенно заменены другими (уровень дохода --- профессия, образование, стаж работы)
  - «ковариаты» могут быть важны при формировании кластеров (число учащихся и учителей в школах)
  - правильный выбор переменных крайне важен
  - критерием при отборе переменных выступает ясность интерпретации полученного результата и «интуиция исследователя»
  
* метод стандартизации
  - качество кластеризации может зависит от выбранного метода стандартизации
  
* выбор метрики для расстояния между объектами
* выбор метрики для расстояния между кластерами
  - если кластеры выражены, то метрика не важна
  - если появляются кластеры сложной формы (например, ленточные), то всё становится сложнее

* [иногда] определение числа кластеров
* интерпретация результатов
  - результаты кластерного анализа нуждаются в интерпретации (если он не решает чисто техническую задачу сокращения размерности данных)
  - лучший результат кластеризации --- это тот, который вы смогли понять и проинтерпретировать





## Расстояние между кластерами

Хорошо, мы поговорили о том, как считать расстояние между объектами, но нам надо понять, насколько (не)похожи получившиеся группы объектов. Для этого придется считать расстояние между кластерами. Варинатов, как обычно, масса.

### Среднее невзвешенное расстояние

**Среднее невзвешенное рассрояние (Average linkage clustering)** определяется так:

* находим расстояния между всеми парами объектов двух кластеров
* усредняем их

КАРТИНКА


### Центроидный метод

Ранее был самым популярным из-за вычислительной простоты. Определяется расстояние между *центрами тяжести* двух кластеров:

КАРТИНКА

В настоящее время используется крайне мало.


### Метод дальнего соседа

Расстояние между кластерами определяется как расстояние между наиболее удалёнными объектами кластеров:

КАРТИНКА


### Метод ближайшего соседа

Расстояние между кластерами определяется как расстояние между наиболее близкими объектами кластеров:

КАРТИНКА

Хорошо работает с ленточными кластерами.

### Расстояние Sørensen---Dice

Довольно экзотичная метрика, но может быть полезна, например, при определии расстояний между сайтами:

$$
Q = \frac{2|A \cap B|}{|A| + |B|}
$$

Могут использоваться его модификации, например:

$$
Q = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}
$$





## Иерархическая кластеризация

### Алгоритм иерархического кластерного анализа

1. Каждый объект объявляется кластером --- из $n$ наблюдений получается $n$ кластеров.
2. Выбираются два ближайших кластера --- они объединяются.
3. Выбираются два ближайших кластера --- они объединяются [2].
4. Выбираются два ближайших кластера --- они объединяются [3].
5. Так происходит до тех пор, пока не остается два кластера.
6. Оставшиеся два кластера являются ближайшими друг с другу --- поэтому объединяются в один.

Звучит, как какой-то сюр --- начали с $n$ кластеров по одному объекту, закончили один кластером, содержащим все объекты… Да, в таком исполнении, действительно, странная процедура. Однако если мы на каком-то этапе её прервём, то получим желаемый результат.

На каком этапе стоит остановиться? Когда расстояния между объединяемыми кластерами становится большим, так как большое расстояние говорит о том, что мы объединяем непохожие объекты.


### Дендрограмма

В иерархическом кластерном анализе есть удобный инструмент для определения момента, когда стоит остановиться в объединении кластеров. Он называется *дендрограмма*. По своей сути, это визуализация алгоритма иерархического кластерного анализа.

Принцип построения дендрограммы следующий:

1. На прямой располагаются все наблюдения как отдельные кластеры.
2. Каждому кластеру соответствует вертикальная линия.
3. Каждому объединению кластеров соответствует горизонтальная линия.
4. Высота, на которой кластеры соединяются, отражает расстояние между кластерами.

Разберемся с этим на примере.

КАРТИНКА

При анализе дендрограммя мы ищем *скачок расстояний*. Он обозначает момент, когда мы перешли к объединению непохожих (далёких друг от друга кластеров). Собственно, это и есть тот момент, когда необхожимо было прервать алгоритм и оставить те кластеры, которые образовались на текущий момент.


### Каменистая осыпь

Ещё один способ определить число кластеров --- это график «каменистая осыпь».

КАРТИНКА

В данном случае по оси $x$ располагаются шаги объединения, по оси $y$ --- расстояние между кластерами в момент объединения. Как именно нам помогает такой график?

В идеальном случае график будет выглядеть так:

КАРТИНКА

Мы видим, что сначала расстояния между объединяемыми кластерами растут медленно, а затем происходит *излом* линнии, и после него расстояния начинаются расти быстро. Это является указанием, что на шаге, где происходит излом линии, необходимо прервать процедуру объединения.


### Когда кластеризации нет?

Как вы понимаете, паттерны дендрограммы и каменистой осыпи могут быть крайне разнообразны в зависимости от того, что есть в данных. Однако главный момент, который нам говорит о том, что кластеризаци нет --- это отсутствие скачка расстояний на дендрограмме и/или отсутствие излома линии на графике «каменистая осыпь».



### Иерархическая кластеризация в R


### Чем плох иерархический кластерный анализ?

У иерархического кластерного анализа практически нет недостатков, за исключением одного очень важного технического --- он требует, чтобы в оперативной памяти хранилась матрица попарных расстояний. Если у нас порядка ста объектов, то проблем никаких, а вот если объектов 100 000, уже возникают трудности. Невозможность работать с очень большими датасетами --- основная проблема этого вида кластерного анализа.



## k-means (метод k-средних)

### Алгоритм метода k-средних

Процедура кластерного анализа этим методом значительно отличается. 

1. Заранее определяется число кластеров $k$. Хотя вообще-то это невозможно, однако уже найдены способы, чтобы обойти это ограничение.
2. Для анализа выбирается $k$ точек --- центры кластеров.
3. Объект приписывается к тому кластеру, чей центр ближайший.
4. Центр кластера --- *центр тяжести* объектов кластера.
  - центр тяжести множества точек с координатами $(x_{i1}, x_{i2}, \dots, x_{ip})$ --- это точка с координатами $(\bar x_{1}, \bar x_{2}, \dots, \bar x_{p})$.
5. Повторяем поочерёдно пункты 3 и 4 до тех пор, пока центры кластеров не перестанут двигаться.


### Ограничения k-means

* Необходимо заранее определить число кластеров
* Используется только евклидово растояние
  - хотя этот недостаток исправляется в других модификациях метода
* Результат зависит от начальных центров кластеров


### Начальное положение кластеров

Если «бросать» центроиды совсем случайно, то это может привести к тому, что некоторые из них буду, например, слишком далеко от скопления точек --- в результате работы алгоритма образуются пустые кластеры. Это нехорошо, поэтому есть два наиболее популярных подхода.

*1. Forgy*
  - Случайным образом выбираются $k$ наблюдений. Они объявляются начальными центрами кластеров.

*2. Случайное разбиение (Random Partition)*
  - Каждое наблюдение случайным образом приписывается к одному из кластеров. Находятся центры тяжести кластеров. Они объявляются начальными центрами кластеров.



### k-means в R





## Интерпретация результатов кластерного анализа

## Метрики качества кластеризации

### Внешние метрики

### Внутренние метрики


## Нечёткий кластерный анализ

